{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8661f582",
   "metadata": {},
   "source": [
    "### Implementation, training and testing of a simple Recurrent Neural Network for time series forecasting\n",
    "\n",
    "In this notebook, we will implement a simple recurrent neural network (RNN) from scratch using only Python and NumPy.\n",
    "\n",
    "A key characteristic of the neural networks you have encountered so far, like densely connected networks and convolutional networks, is their lack of memory. They process each input independently, without retaining any state across inputs. To handle sequences or time-series data using such networks, you must present the entire sequence at once, treating it as a single data point. For instance, as you’re reading this text, your brain is processing its content word by word while keeping memories of what was read before. This way, you can have a fluid representation of the overall content.\n",
    "\n",
    "\n",
    "RNNs adopt the same principle. Basically, RNNs are a type of artificial neural network designed to work with sequential data. Unlike traditional feedforward neural networks, RNNs have internal connections that loop back, allowing information to persist across time steps. This makes RNNs particularly suited for tasks such as time series prediction, natural language processing, and sequence classification. In practice, a single sequence of $N$ elements will be considered as a single input to the RNN. A standard DNN would take that input as an $N$ size input and process all the elements at once in a single step. What changes in an RNN is that this single input is no longer processed in a single step, but the RNN loops over sequence elements internally.\n",
    "\n",
    "An RNN consists of the following components:\n",
    "\n",
    "Input Layer $\\rightarrow$ Processes the sequential data.\n",
    "\n",
    "Hidden Layer $\\rightarrow$ Contains neurons with internal connections to model temporal dependencies.\n",
    "\n",
    "Output Layer $\\rightarrow$ Produces predictions.\n",
    "\n",
    "The output at a time $t$ is defined as\n",
    "$$\n",
    "y_t = \\mathbf{W}_y\\cdot s_t + b_y\n",
    "$$\n",
    "with $\\mathbf{W}_y$ the matrix of weights for the output, $b_y$ the bias for the output and $s_t$ the internal state at time $t$, given by\n",
    "$$\n",
    "s_t = \\tanh\\left(\\mathbf{W}_x \\cdot x_t + \\mathbf{W}_s \\cdot s_{t-1} + b_s\\right)\n",
    "$$\n",
    "where $x_t$ is the input at time t, $s_{t-1}$ is the internal state at time $t-1$, $\\mathbf{W}_x$ is the matrix of weights for the input, $\\mathbf{W}_s$ is the matrix of weights for the internal state and $b_s$ is the bias for the internal state. Here, we have assumed a $\\tanh$ activation function, but other types of activation function are possible.\n",
    "\n",
    "In this notebook, we will code and train a simple RNN to predict a shifted sequence given a sequence of $n$ previous steps, i.e.\n",
    "$$\n",
    "(x_{t-n}, x_{t-(n-1)},\\dots,x_{t-1})\\rightarrow RNN \\rightarrow (x_{t-(n-1)},x_{t-(n-2)},\\dots,x_{t})\n",
    "$$\n",
    "\n",
    "This is called sequence-to-sequence coding, it is a very simple case, but still essential for time series forecasting. Indeed, the prediction at $t$ provided by the RNN will be used as an input for the next step for the RNN to predict the output at $t+1$ and so on. This is called \"iterative prediction\", but in this Notebook we will focus only on the prediction of a single time step given a time sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a1ca2d",
   "metadata": {},
   "source": [
    "First of all, let us import the essential modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a68ce76",
   "metadata": {},
   "source": [
    "Now, we will generate synthetic data, using for example a sine wave. We will generate a given number of samples. Each sample will contain a sequence $x$ with a number of elements and a sequence $y$ with the same number of elements, equal to $x$ but shifted by one time step forward, with the same number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sine_wave(seq_length, num_samples):\n",
    "    X = []\n",
    "    y = []\n",
    "    for _ in range(num_samples):\n",
    "        start = np.random.rand() * 2 * np.pi\n",
    "        x_seq = np.sin(np.linspace(start, start + 2 * np.pi, seq_length + 1))\n",
    "        ...................  # Input sequence\n",
    "        ...................   # Target sequence\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fe70a8",
   "metadata": {},
   "source": [
    "Now we will use the previously coded function to generate the synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ff0cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 10\n",
    "num_samples = 1000\n",
    "X, y = ...................\n",
    "#Check the shape and length of the arrays\n",
    "...................\n",
    "...................\n",
    "...................\n",
    "..................."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c709018",
   "metadata": {},
   "source": [
    "The next cell contains the class that defines the RNN. It contains in particular the forward method (given an $x$ sequence, predict the time-shifted $y$ sequence) and the backward method to update the trainable parameters using a simple gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e056f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Class\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.Wx = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.Ws = ...................\n",
    "        self.Wy = ...................\n",
    "        self.bs= np.zeros((..................., 1))\n",
    "        self.by = np.zeros((..................., 1))\n",
    "\n",
    "        self.intermediate_values = []  # Store intermediate values for backpropagation\n",
    "\n",
    "    def forward(self, x_seq):\n",
    "        s = np.zeros((self.hidden_size, 1))\n",
    "        self.intermediate_values = []  # Clear the intermediate values for a new sequence\n",
    "        y_pred = []\n",
    "\n",
    "        for x_t in x_seq:\n",
    "            x_t = x_t.reshape(-1, 1)  # Ensure x_t is a column vector\n",
    "            s = ...................\n",
    "            y_t = ...................\n",
    "            y_pred.append(y_t)\n",
    "            self.intermediate_values.append((s, x_t))  # Save internal state and input for backpropagation\n",
    "\n",
    "        return np.array(y_pred).squeeze()\n",
    "\n",
    "    # The next method computes the gradients of the loss function with respect to the\n",
    "    # parameters of the model (Wx, Ws, Wy, bs, by) and updates them using gradient descent\n",
    "    def backward(self, dLdy, learning_rate):\n",
    "        dWx = np.zeros_like(self.Wx)\n",
    "        dWs = np.zeros_like(self.Ws)\n",
    "        dWy = np.zeros_like(self.Wy)\n",
    "        dbs = np.zeros_like(self.bs)\n",
    "        dby = np.zeros_like(self.by)\n",
    "        ds_next = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # Loop Through Timesteps in Reverse\n",
    "        # RNNs backpropagate through time, i.e.\n",
    "        # it iterates backward through the sequence\n",
    "        # to compute gradients at each time step\n",
    "        for t in reversed(range(len(self.intermediate_values))):\n",
    "            # Retrieve intermediate values values\n",
    "            s, x_t = self.intermediate_values[t]\n",
    "            \n",
    "            # Compute gradient of output weights and biases\n",
    "            # dLdy_t is the loss gradient for the output at time t\n",
    "            dLdy_t = dLdy[t].reshape(-1, 1)  # Ensure dy is a column vector\n",
    "            dWy += np.dot(dLdy_t, s.T)\n",
    "            dby += dLdy_t\n",
    "            \n",
    "            #Backpropagate to the internal state\n",
    "            ds = np.dot(self.Wy.T, dLdy_t) + ds_next\n",
    "            # ds_tanh applies the derivative of the tanh activation function, (1−s^2)\n",
    "            ds_tanh = ................... * ds\n",
    "\n",
    "            # Compute gradients for the input and the internal state weights and biases\n",
    "            dWx += np.dot(ds_tanh, x_t.T)\n",
    "            dWs += np.dot(ds_tanh, self.intermediate_values[t - 1][0].T) if t > 0 else 0\n",
    "            dbs += ds_tanh\n",
    "            \n",
    "            # Update the gradient for the next time step\n",
    "            ds_next = np.dot(self.Ws.T, ds_tanh)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.Wx -= ...................\n",
    "        self.Ws -= ...................\n",
    "        self.Wy -= ...................\n",
    "        self.bs -= ...................\n",
    "        self.by -= ..................."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e17bc79",
   "metadata": {},
   "source": [
    "The next step is the training of the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2bf8c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the RNN\n",
    "# Select a seed for the random generator just to ensure that the results can be reproduced (for debugging purposes)\n",
    "np.random.seed(42)\n",
    "#Initialize the RNN\n",
    "...................\n",
    "\n",
    "#Select the number of epochs\n",
    "epochs = ...................\n",
    "\n",
    "total_loss = []\n",
    "\n",
    "#Select a learning rate\n",
    "learning_rate = ...................\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(len(X)):\n",
    "        x_seq = X[i]\n",
    "        y_true = y[i]\n",
    "\n",
    "        # Complete the forward pass\n",
    "        y_pred = ...................\n",
    "\n",
    "        # Compute the loss using the mean equared error (MSE)\n",
    "        loss = ...................\n",
    "        \n",
    "        #Add the loss to the epoch_loss\n",
    "        ...................\n",
    "\n",
    "        # Backward pass\n",
    "        # Compute the gradient of the MSE loss with respect to the prediction y_pred\n",
    "        ...................\n",
    "        # Update the model parameters using the backward method of the RNN class\n",
    "        ...................\n",
    "    \n",
    "    # Append the obtained loss to the total_loss list, normalizing by the number of samples\n",
    "    ...................\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a6bf90",
   "metadata": {},
   "source": [
    "Let us now visualize the training loss as a function of the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Loss Curve\n",
    "..................."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1349cd",
   "metadata": {},
   "source": [
    "Let us test our trained RNN on a single time sequence (num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1ddd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RNN\n",
    "# Visualize predictions on a new sine wave.\n",
    "..................."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2361efc8",
   "metadata": {},
   "source": [
    "Plot the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the input sequence and the predicted sequence.\n",
    "# The predicted one is supposed to be the input sequence, but shifted by one time step forward\n",
    "..................."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6cd357",
   "metadata": {},
   "source": [
    "If you have time, you can try to code the iterative prediction, which means that you use now the predicted value as an input for the RNN to predict the next step and so on. What do you observe?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
