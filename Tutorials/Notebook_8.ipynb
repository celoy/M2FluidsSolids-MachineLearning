{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93579ba4",
   "metadata": {},
   "source": [
    "In this notebook, we will implement simple MLP using NumPy.\n",
    "After this, you will understand the fundamental concepts behind training and evaluating neural networks, i.e. forward propagation, backward propagation and gradient descent. You will also analyse the impact of the hyper-parameters on the performance of the training and the generalization of the model.\n",
    "\n",
    "First of all, let us import the essential modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dae432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba745f49",
   "metadata": {},
   "source": [
    "Let us start with a neural network used for binary classification.\n",
    "For this, we create a synthetic dataset with two classes. We use matplotlib to plot the dataset in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.randn(400, 2)  # 300 data points with 2 features\n",
    "y = (X[:, 0]**2 + X[:, 1]**2 < 1).astype(int)  # Circular decision boundary\n",
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb72b3",
   "metadata": {},
   "source": [
    "### Preliminary question\n",
    "Plot the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993f4d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write here your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720e7046",
   "metadata": {},
   "source": [
    "We use the train_test_split function from Scikit-learn to split the dataset into train and test datasets. We use typically 80%-20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d66e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shape of the resulting datasets\n",
    "print(f\"Training set: X_train shape = {X_train.shape}, y_train shape = {y_train.shape}\")\n",
    "print(f\"Testing set: X_test shape = {X_test.shape}, y_test shape = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a38218b",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "How many inputs does our neural network require?\n",
    "How many outputs does our neural network provide?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cdf7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = ....... # Number of features\n",
    "output_dim = ....... # Number of output units (binary classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d380c44d",
   "metadata": {},
   "source": [
    "We build a neural network with a given number of hidden layers. For the moment, we will use a single hidden layer with four neurons. Later, we will work on the impact of this number of the training.\n",
    "### Question 2\n",
    "Fill in the cell below with the missing information, using a list of integers to provide the architecture of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = ......."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5fed2e",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Before training the model, we need to initialize the weights and biases randomly. Fill in the cell below with the missing information to initialize the weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52ded5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Initialize parameters for a neural network with multiple layers.\n",
    "    Arguments:\n",
    "    - layer_dims: List containing the dimensions of each layer.\n",
    "\n",
    "    Returns:\n",
    "    - parameters: Dictionary containing initialized weights and biases for all layers.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    parameters = {}\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        parameters[f\"W{l}\"] = .......\n",
    "        parameters[f\"b{l}\"] = .......\n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters(layer_dims)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b31611",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Write a Python function for a sigmoid activation function and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f9836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    .......\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_derivative(z):\n",
    "    ......."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf17b95",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Write a Python function for a ReLU activation function and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca576bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    .......\n",
    "\n",
    "def relu_derivative(z):\n",
    "    ......."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbdbd0b",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Now we need to write a function to evaluate the neural network. This is the so-called forward pass. Fill in the cell below with the missing information so that each neuron produces as output W.x+b, with x the output of the previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9680751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    activations = {'A0': X}  # Input layer activation\n",
    "    L = len(parameters) // 2  # Number of layers (parameters include W1, b1, ..., WL, bL)\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        Z = .......\n",
    "        if l == L:  # Output layer\n",
    "            activations[f\"A{l}\"] = .......\n",
    "        else:  # Hidden layers\n",
    "            activations[f\"A{l}\"] = .......\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa62811",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "The most important part of training a model is the backward pass, by which the parameters are updated using the gradient and the learning rate. Fill in the cell below with the missing information to update the parameters following a gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc09fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(activations, y, parameters, learning_rate=0.01):\n",
    "    m = y.shape[0]  # Number of training examples\n",
    "    gradients = {}\n",
    "    L = len(parameters) // 2  # Number of layers\n",
    "\n",
    "    # Output layer gradients\n",
    "    dZ = activations[f\"A{L}\"] - y\n",
    "    gradients[f\"dW{L}\"] = np.dot(activations[f\"A{L-1}\"].T, dZ) / m\n",
    "    gradients[f\"db{L}\"] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "\n",
    "    # Backpropagate through hidden layers\n",
    "    for l in range(L-1, 0, -1):\n",
    "        dA = np.dot(dZ, parameters[f\"W{l+1}\"].T)\n",
    "        dZ = dA * relu_derivative(activations[f\"A{l}\"])\n",
    "        gradients[f\"dW{l}\"] = np.dot(activations[f\"A{l-1}\"].T, dZ) / m\n",
    "        gradients[f\"db{l}\"] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "\n",
    "    # Update parameters\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[f\"W{l}\"] ....... ##### <-- Complete this line\n",
    "        parameters[f\"b{l}\"] ....... ##### <-- Complete this line\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50db469c",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "Once the forward and backward passes are correctly implemented. The idea is to train the model iteratively. Each iteration is called epoch. Fill in the cell below with the missing information. Choose in particular a number of epochs and a value for the learning rate. Select different number of epochs and learning rate and describe what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb82e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = .......\n",
    "learning_rate = .......\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward propagation on training data\n",
    "    activations = .......  \n",
    "    A_train_final = activations[f\"A{len(layer_dims)-1}\"]  # Output layer activation on training data\n",
    "    \n",
    "    # Training loss\n",
    "    train_loss = .......\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Forward propagation on validation data\n",
    "    val_activations = .......\n",
    "    A_val_final = val_activations[f\"A{len(layer_dims)-1}\"]  # Output layer activation on validation data\n",
    "    \n",
    "    # Validation loss\n",
    "    val_loss = .......\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Backward propagation on training data\n",
    "    parameters = backward_propagation(.......)\n",
    "    \n",
    "    # Print training and validation loss every 100 epochs\n",
    "    ......."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36d627",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "The best way to assess the performance of the training and the model is by plotting as a function of the epochs the training and the validation losses. Write the Python lines in the cell below to plot these two curves for different values of epochs and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3fbc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f86733",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "Perform several training, increasing the complexity of the model. You can do that by increasing the number of neurons in each hidden layer and/or the number of hidden layers. Plot the training/validation losses as a function of the number of epochs. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b69e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill this cell with your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bc004e",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "Now we will use different trained models and will predict the decision boundary. For this, we will first write a function that plots the decision boundary for a given model. Fill in the cell below with the missing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d1f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(forward_propagation, X, y, parameters, layer_dims):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    # Grid of points to evaluate the model\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, 0.01),\n",
    "        np.arange(y_min, y_max, 0.01)\n",
    "    )\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]  # Flatten and combine the meshgrid into input data shape\n",
    "    \n",
    "    # Predict for each point on the grid\n",
    "    activations = .......\n",
    "    predictions = activations[f\"A{len(layer_dims) - 1}\"]  # Final output layer activations\n",
    "    predictions = .......  # Convert probabilities to binary labels\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    plt.contourf(xx, yy, predictions.reshape(xx.shape), alpha=0.7, cmap=\"viridis\")\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y.flatten(), edgecolors=\"k\", cmap=\"viridis\")\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the plot_decision_boundary function\n",
    "......."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187689bc",
   "metadata": {},
   "source": [
    "The example we have considered is very simple and one would like to apply this to more complex boundaries. Let us therefore consider the examples of the moons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5107a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=1000, noise=0.15, random_state=0)\n",
    "y = y.reshape(-1, 1)  # Reshape to match the required format\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y.flatten(), cmap=\"viridis\")\n",
    "plt.title(\"Dataset: Moons with Noise\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4fc6f",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "Perform the previous steps for this dataset. Compare the performance of the same model trained and evaluated with the synthetic dataset that we generated at the beginning and with the moon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b1cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write here your answer(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
