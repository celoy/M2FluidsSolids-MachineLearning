{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "In this lab session, we will code a backpropagation algorithm. This backpropagation algorithm is needed to implement a gradient descent method for a neural network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to be fitted\n",
    "\n",
    "We want to approach the following function with a neural network\n",
    "$$f(x_1,x_2) = x_2 + (1 - 2 x_1 - x_1^2) \\sin(\\pi x_2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_deterministic(x):\n",
    "    f = x[2] + (1 - 2*x[1] - x[1]**2) * np.sin(np.pi * x[2])\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('$x_1$', fontsize=16)\n",
    "plt.ylabel('$x_2$', fontsize=16)\n",
    "x1 = np.arange(0, 1, 0.01)\n",
    "x2 = np.arange(0, 1, 0.01)\n",
    "xx1, xx2 = np.meshgrid(x1, x2)\n",
    "xx0 = np.ones(xx1.shape)\n",
    "xx = np.stack((xx0, xx1, xx2), axis=0)\n",
    "z = f_deterministic(xx)\n",
    "h = plt.contourf(x1,x2,z,100)\n",
    "cbar = plt.colorbar(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network\n",
    "We build a neural network with 4 layers. Each layer has $N_l$ neurons. We use the notations $p^{(l)}$ and $o^{(l)}$ for the predecessor and output vectors of neurons in layer $l$ ($l=0\\ldots L$). The matrices of synaptic weights are noted $\\Theta^{(l)}$. \n",
    "\n",
    "### Feedforward calculation\n",
    "The feedforward calculation aims at calculating $h_\\theta(x)$ using the following feedforward procedure\n",
    "\\begin{align}\n",
    "o^{(0)} & = x \\\\\n",
    "p^{(1)} & = \\Theta^{(0)}\\cdot o^{(0)}\\\\\n",
    "o^{(1)} & = g\\left( p^{(1)} \\right) \\\\\n",
    "p^{(2)} & = \\Theta^{(1)}\\cdot o^{(1)}\\\\\n",
    "o^{(2)} & = g\\left( p^{(2)} \\right) \\\\\n",
    "p^{(3)} & = \\Theta^{(2)}\\cdot o^{(2)}\\\\\n",
    "o^{(3)} & = p^{(3)} \\\\\n",
    "h_\\theta(x) & = o^{(3)}\n",
    "\\end{align}\n",
    "where $x=[1, x_1, x_2]$ and $g$ is a non-linear \"sigmoid\" function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of neurons in each layer\n",
    "N0 = 3  # INPUT (x0 = 1, x1, x2)\n",
    "N1 = 8\n",
    "N2 = 8\n",
    "N3 = 1  # OUTPUT\n",
    "Ns = (N0, N1, N2, N3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLfunction(p):\n",
    "    f = np.maximum(0,p)\n",
    "    fprime = 0.5 + 0.5*np.sign(p)\n",
    "    # f = np.tanh(p)\n",
    "    # fprime = 1 - np.tanh(p)**2\n",
    "    # f = 1/(1+np.exp(-p))\n",
    "    # fprime = f * (1-f)\n",
    "    return f, fprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(Thetas, x):\n",
    "    o0 = x\n",
    "    p1 = Thetas[0] @ o0\n",
    "    o1, gp1 = NLfunction(p1)\n",
    "    p2 = Thetas[1] @ o1\n",
    "    o2, gp2  = NLfunction(p2)\n",
    "    p3 = Thetas[2] @ o2\n",
    "    o3 = p3\n",
    "    gp3 = 1\n",
    "    \n",
    "    os = (o0, o1, o2, o3)\n",
    "    gps= (1, gp1, gp2, gp3)\n",
    "    h = o3\n",
    "    return h, os, gps    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random initialisation of matrices\n",
    "Theta0 = np.random.normal(scale=1.0,size=(Ns[1],Ns[0]))\n",
    "Theta1 = np.random.normal(scale=1.0,size=(Ns[2],Ns[1]))\n",
    "Theta2 = np.random.normal(scale=1.0,size=(Ns[3],Ns[2]))\n",
    "Thetas = (Theta0, Theta1, Theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Plot the output of the NN for $x_1 \\in [0,1]$ and $x_2 \\in [0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation algorithm\n",
    "\n",
    "As we have seen in class, the backpropagation algorithm consists first in calculating the vectors $\\delta^{(l)}$ for each layer starting by the output layer $l=L=2$ and calculating $\\delta$'s for each layer recursively. \n",
    "\n",
    "\\begin{align}\n",
    "\\delta^{(L)} & = (h_\\theta(x) - y) g'(p^{(L)}) \\\\\n",
    "\\delta^{(l)} & = \\left(\\Theta^{(l)}\\right)^T \\delta^{(l+1)} .* g'\\left(p^{(l)}\\right)\n",
    "\\end{align}\n",
    "\n",
    "Once these vectors $\\delta$ are calculated the gradients are obtained with this formula\n",
    "$$D^{(l)}_{ij}\\equiv \\frac{\\partial E}{\\partial \\Theta_{ij}^{(l)}} = \\delta_i^{(l+1)} o_j^{(l)}$$\n",
    "where $E=\\tfrac{1}{2} \\left( h_\\theta(x) - y \\right)^2$ is the quadratic error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT\n",
    "x = np.random.rand(3)\n",
    "x[0]=1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Compute the matrices D0, D1 and D2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the matrices here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification of backpropagation algorithm\n",
    "We want to assess whether the calculation of the gradient ${\\partial E}\\,/\\,{\\partial \\Theta_{ij}^{(l)}}$ is correct. To do so, we will compare a component of the matrix $D^{(l)}$ with an approximate value of the gradient obtained by calculating \n",
    "$$\\tilde D_{ij} = \\frac{E(\\Theta_{ij} + \\epsilon) - E(\\Theta_{ij})}{\\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Calculate the matrices Dtilde0, Dtilde1 and Dtidle2 and compare them with D0, D1, D2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the matrices here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "We recall the principle of stochastic gradient descent. It is a loop where the following actions are performed. \n",
    "- Pick a data point $(x^{(i)},y^{(i)})$\n",
    "- Calculate the gradient matrices $D^{(l)}$ for this point\n",
    "- Update the matrices $\\Theta^{(l)}$\n",
    "$$\\Theta^{(l)} := \\Theta^{(l)} - \\alpha \\,D^{(l)}$$\n",
    "where $\\alpha$ is the learning parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Code the descent gradient algorithm with J = (h - yi), the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01      # Learning parameter\n",
    "Nsamples = 100000  # Number of samples\n",
    "\n",
    "# Training set\n",
    "x = np.random.rand(3, Nsamples)\n",
    "x[0] = np.ones(Nsamples)\n",
    "y = f_deterministic(x)\n",
    "\n",
    "# Random initialisation of matrices\n",
    "Theta0 = np.random.normal(scale=1.0,size=(Ns[1],Ns[0]))\n",
    "Theta1 = np.random.normal(scale=1.0,size=(Ns[2],Ns[1]))\n",
    "Theta2 = np.random.normal(scale=1.0,size=(Ns[3],Ns[2]))\n",
    "Thetas = (Theta0, Theta1, Theta2)\n",
    "\n",
    "J = np.zeros(Nsamples)\n",
    "error = 1\n",
    "for t in range(Nsamples):\n",
    "    # TODO : put your code here, use with J = (h - yi) for the error\n",
    "\n",
    "\n",
    "    J[t] = np.random.randn()\n",
    "\n",
    "\n",
    "# FIGURES : plot the real function, the network approximation and the error evolution   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent\n",
    "\n",
    "Now, we want to implement a mini-batch gradient descent, where at each step, $N_{batch}$ samples are drawn at random from the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Code the mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05       # Learning parameter\n",
    "Nsamples = 100000  # Number of samples\n",
    "Nbatch = 16        # Size of mini-batch\n",
    "Nstep = 32000      # Number of mini-batches for training\n",
    "\n",
    "# Random initialisation of matrices\n",
    "Theta0 = np.random.normal(scale=1.0,size=(Ns[1],Ns[0]))\n",
    "Theta1 = np.random.normal(scale=1.0,size=(Ns[2],Ns[1]))\n",
    "Theta2 = np.random.normal(scale=1.0,size=(Ns[3],Ns[2]))\n",
    "Thetas = (Theta0, Theta1, Theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random initialisation of matrices\n",
    "Theta0 = np.random.normal(scale=1.0,size=(Ns[1],Ns[0]))\n",
    "Theta1 = np.random.normal(scale=1.0,size=(Ns[2],Ns[1]))\n",
    "Theta2 = np.random.normal(scale=1.0,size=(Ns[3],Ns[2]))\n",
    "Thetas = (Theta0, Theta1, Theta2)\n",
    "\n",
    "J = np.zeros(Nstep)\n",
    "error = 1\n",
    "for t in range(Nstep):\n",
    "# Your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyPython",
   "language": "python",
   "name": "mypython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
