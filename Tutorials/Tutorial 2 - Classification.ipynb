{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Lesson 7 - Classification: Logistic regression, k-means\n",
    "\n",
    "In this hands-on lesson, we will: implement three different algorithms for classification.  \n",
    "- Implement Logistic regression from scratch and apply it to a very simple dataset.\n",
    "- Using sklearn and logistic regression to classify digits\n",
    "- Implement K-means from scratch and apply it to a very simple dataset.\n",
    "- Using sklearn and decision trees for simple classificion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To start, let us import the required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Reminder about Logistic Regression\n",
    "Logistic regression is a supervised learning algorithm for classification and it works as follows:\n",
    " \n",
    "- the given dataset is: $\\{(\\boldsymbol{x}^{(1)}, y^{(1)}), ..., (\\boldsymbol{x}^{(N)}, y^{(N)})\\}$  (it contains $N$ samples) \n",
    "- each $\\boldsymbol{x}^{(i)}$ is a $m-$dimensional vector $\\boldsymbol{x}^{(i)} = (x^{(i)}_1, ..., x^{(i)}_m)$\n",
    "- the outcomes $y^{(i)}$ are binary target variables, $y^{(i)} \\in \\{0,1\\}$\n",
    "\n",
    "The logistic regression model can be interpreted as a very **simple neural network:**\n",
    "\n",
    "![title](Figures/LogisticRegression.png)\n",
    "\n",
    "- it has  only one input layer connected with the output via a real-valued weight vector $\\boldsymbol{w}= (w^{(0)}, w^{(1)}, ..., w^{(m)})$. \n",
    "- it uses a **sigmoid activation function**\n",
    "- weights can be trained using  **gradient descent**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equations for Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement Logistic regression we need to perform the following steps, which we break into parts:\n",
    "\n",
    "0. Initialize the weight vector and bias with zeros (or small random values).\n",
    "\n",
    "1. Compute a linear combination of the input features and weights. Denoting with $\\boldsymbol{X}$ the matrix of shape $(N,m)$ that holds all training examples (N= number of samples, m = number of features),\n",
    "this can be done in one step for all training examples, using vectorization and broadcasting:\n",
    "\n",
    "$$\\boldsymbol{z} = \\boldsymbol{X} \\boldsymbol{w} + b $$\n",
    "\n",
    "2. Apply the sigmoid activation function, which returns values between 0 and 1:\n",
    "\n",
    "$$\\boldsymbol{\\hat{y}} = \\sigma(\\boldsymbol{z}) = \\frac{1}{1 + \\exp(-\\boldsymbol{z})}$$\n",
    "\n",
    "3. Compute the cost over the whole training set. \n",
    "\n",
    "$$J(\\boldsymbol{w}) = - \\frac{1}{N} \\sum_{i=1}^N \\Big[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\Big]$$\n",
    "\n",
    "4. Compute the gradient of the cost function with respect to the weight vector:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w_j} = \\frac{1}{N}\\sum_{i=1}^N\\left[\\hat{y}^{(i)}-y^{(i)}\\right]\\,x_j^{(i)} $$\n",
    "\n",
    "5. Update the weights ($\\alpha$ is the learning rate):\n",
    "\n",
    "$$\\boldsymbol{w} = \\boldsymbol{w} - \\alpha \\, \\nabla_w J$$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Implement Logistic Regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate fake data\n",
    "Note that they are linearly sepearable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will perform logistic regression using a simple toy dataset of two classes\n",
    "np.random.seed(123) # fix the seed for reproducibility\n",
    "\n",
    "X, y_true = make_blobs(n_samples= 1000, centers=2)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:,0], X[:,1], c=y_true)\n",
    "plt.title(\"Dataset\")\n",
    "plt.xlabel(\"First feature\")\n",
    "plt.ylabel(\"Second feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we reshape the target and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape targets to get column vector with shape (N, 1), where N is the number of samples\n",
    "y_true = y_true[:, np.newaxis]\n",
    "\n",
    "# The sklearn function 'train_test_split' to split the data, default splitting is 75% training, 25% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_true) \n",
    "\n",
    "print(f'Shape X_train: {X_train.shape}')\n",
    "print(f'Shape y_train: {y_train.shape}')\n",
    "print(f'Shape X_test: {X_test.shape}')\n",
    "print(f'Shape y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: write a class that implements Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionScratch:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # z is a np array of size (N,1)\n",
    "        return # <- EDIT THIS: it should return the sigmoid function\n",
    "\n",
    "    def train(self, X, y_true, n_iters, learning_rate):\n",
    "        \"\"\"\n",
    "        Trains the logistic regression model on given data X and targets y\n",
    "        \"\"\"\n",
    "        # Step 0: Initialize the parameters\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros((n_features, 1))\n",
    "        #self.bias = 0\n",
    "        costs = []\n",
    "        \n",
    "        for i in range(n_iters):\n",
    "            # Step 1 and 2: Compute a linear combination of the input features and weights, \n",
    "            # apply the sigmoid activation function\n",
    "            y_prob = [] # <- EDIT THIS\n",
    "            \n",
    "            # Step 3: Compute the cost over the whole training set.\n",
    "            cost = [] # <- EDIT THIS\n",
    "\n",
    "            # Step 4: Compute the gradients\n",
    "            dw = [] # <- EDIT THIS\n",
    "            # Step 5: Update the parameters\n",
    "            self.weights = [] # <- EDIT THIS\n",
    "            \n",
    "\n",
    "            costs.append(cost)\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Cost after iteration {i}: {cost}\")\n",
    "\n",
    "        #return self.weights, self.bias, costs\n",
    "        return self.weights,  costs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts binary labels for a set of examples X.\n",
    "        \"\"\"\n",
    "        y_prob = [] # <- EDIT THIS\n",
    "        \n",
    "        y_predict_labels = [1 if elem > 0.5 else 0 for elem in y_prob]\n",
    "\n",
    "        return np.array(y_predict_labels)[:, np.newaxis]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Plot the evolution of cost during training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bias = np.concatenate((np.ones((X_train.shape[0],1)),X_train),axis=1) # add a columns of ones to X\n",
    "\n",
    "regressor = [] # EDIT THIS: call an istantiation of the class\n",
    "w_trained, costs = [], [] # EDIT THIS: train the class with 600 iterations and learning rate 0.009\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "# EDIT HERE: add a line to plot the evolution of the cost\n",
    "\n",
    "plt.title(\"Development of cost over training\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE : plot the decision boundary\n",
    "In the code above we implemented logistic regression using X[:,0], X[:,1] as features.\n",
    "- Which type of decision boundary do you expect to get?\n",
    "- Plot it in the feature1/feature2 plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define x_db and y_db appropriately in order to get the boudaries\n",
    "\n",
    "x_db = [] # <- EDIT THIS\n",
    "y_db = [] # <- EDIT THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=np.squeeze(y_true) # this is to use y_true in the scatter plot\n",
    "\n",
    "# Make the figure\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:,0], X[:,1], c=y_true)\n",
    "plt.plot(x_db,y_db,color='Red',linewidth=3) \n",
    "plt.title(\"Dataset\")\n",
    "plt.xlabel(\"First feature\")\n",
    "plt.ylabel(\"Second feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Apply Logistic Regression to a small digits set using sklearn\n",
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us plot a few digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, label) in enumerate(zip(digits.data[0:5], \n",
    "                                           digits.target[0:5])):\n",
    "    plt.subplot(1, 5, index + 1)\n",
    "    plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray, interpolation='bilinear')\n",
    "    plt.title('Training: %i\\n' % label, fontsize = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let us build our training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, \n",
    "                                                    digits.target,\n",
    "                                                   test_size=0.25,\n",
    "                                                   random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: how many test example are we going to use? Print the number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code create an instantiation of the sklean class for logistic regression and uses it to predict the label of hand-written digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression(fit_intercept=True,\n",
    "                        multi_class='auto',\n",
    "                        penalty='l2', #ridge regression\n",
    "                        solver='saga',\n",
    "                        max_iter=10000,\n",
    "                        C=50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate predictions for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logisticRegr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the misclassified imagesâ€™ index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "misclassifiedIndexes = []\n",
    "for label, predict in zip(y_test, predictions):\n",
    "    if label != predict: \n",
    "        misclassifiedIndexes.append(index)\n",
    "    index +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassifiedIndexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: which is the accuracy of the logistic regression classifier?\n",
    "Reember that the accuracy $A$ is defined as $A$ = (number of correct predictions)/(number of total predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the score attribute of the sklearn logistic regression class to compute the accuracy (check that you obtain the same result)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = logisticRegr.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: plot the first 4 mispredicted images, write a title with actual and predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "# WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means is a very simple clustering algorithm (clustering belongs to unsupervised learning). Given a fixed number of clusters (denoted by $k$) and an input dataset the algorithm tries to partition the data into clusters such that the clusters have high intra-class similarity and low inter-class similarity. \n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Initialize the cluster centers, either randomly within the range of the input data or (recommended) with some of the existing training examples\n",
    "\n",
    "2. Until convergence  \n",
    "\n",
    "   2.1. Assign each datapoint to the closest cluster. The distance between a point and cluster center is measured using the Euclidean distance.  \n",
    "\n",
    "   2.2. Update the current estimates of the cluster centers by setting them to the mean of all instance belonging to that cluster  \n",
    "   \n",
    "   \n",
    "### Objective function\n",
    "\n",
    "The underlying objective function tries to find cluster centers such that, if the data are partitioned into the corresponding clusters, distances between data points and their closest cluster centers become as small as possible.\n",
    "\n",
    "Given a set of datapoints ${x_1, ..., x_n}$ and a positive number $k$, find the clusters $C_1, ..., C_k$ that minimize\n",
    "\n",
    "\\begin{equation}\n",
    "J = \\sum_{i=1}^n \\, \\sum_{j=1}^k \\, z_{ij} \\, || x_i - \\mu_j ||_2\n",
    "\\end{equation}\n",
    "\n",
    "where:  \n",
    "- $z_{ij} \\in \\{0,1\\}$ defines whether of not datapoint $x_i$ belongs to cluster $C_j$\n",
    "- $\\mu_j$ denotes the cluster center of cluster $C_j$\n",
    "- $|| \\, ||_2$ denotes the Euclidean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate simple data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "X, y = make_blobs(centers=4, n_samples=1000)\n",
    "print(f'Shape of dataset: {X.shape}')\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.title(\"Dataset with 4 clusters\")\n",
    "plt.xlabel(\"First feature\")\n",
    "plt.ylabel(\"Second feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans():\n",
    "    def __init__(self, n_clusters=4):\n",
    "        self.k = n_clusters\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        Fits the k-means model to the given dataset\n",
    "        \"\"\"\n",
    "        n_samples, _ = data.shape\n",
    "        # initialize cluster centers\n",
    "        self.centers = [] # <- EDIT HERE\n",
    "        self.initial_centers = [] # <- EDIT HERE\n",
    "        # We will keep track of whether the assignment of data points\n",
    "        # to the clusters has changed. If it stops changing, we are \n",
    "        # done fitting the model\n",
    "        old_assigns = None\n",
    "        n_iters = 0\n",
    "\n",
    "        self.centers_memory = [] # <- EDIT HERE\n",
    "        self.data_classification_memory = [] # THIS MUST NOT BE MODIFIED\n",
    "        \n",
    "        while True:\n",
    "            new_assigns = [self.classify(datapoint) for datapoint in data]\n",
    "            self.data_classification_memory.append(new_assigns)\n",
    "            if new_assigns == old_assigns:\n",
    "                print(f\"Training finished after {n_iters} iterations!\")\n",
    "                return\n",
    "\n",
    "            old_assigns = new_assigns\n",
    "            n_iters += 1\n",
    "\n",
    "            # recalculate centers\n",
    "            for id_ in range(self.k):               \n",
    "            # EDIT THIS: write the code that recalculates the centers,it requires 2 lines of code\n",
    "                self.centers[id_] = [] # EDIT THIS: to return the new centers\n",
    "            self.centers_memory.append(self.centers)\n",
    "    def l2_distance(self, datapoint):\n",
    "        dists = [] # EDIT THIS: it should return the eucledian distance between the datapoint and all the centers\n",
    "        return dists\n",
    "\n",
    "    def classify(self, datapoint):\n",
    "        \"\"\"\n",
    "        Given a datapoint, compute the cluster closest to the\n",
    "        datapoint. Return the cluster ID of that cluster.\n",
    "        \"\"\"\n",
    "        dists = [] # <- EDIT HERE: compute the distance using the attribute l2_distance\n",
    "        return # <- EDIT HERE: assign each point to a given cluster, hit use: np.argmin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the class you wrote to implement the k-means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NITERATIONS = len(kmeans.centers_memory) \n",
    "plt.figure(figsize=(4*NITERATIONS,4))\n",
    "for i in range(NITERATIONS):\n",
    "    plt.subplot(1,NITERATIONS,i+1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], marker='.', c=kmeans.data_classification_memory[i])\n",
    "    plt.scatter(kmeans.centers_memory[i][:,0], kmeans.centers_memory[i][:,1], c='r')\n",
    "    plt.title('Iteration # : {}'.format(i), fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: use the elbow method to check that the correct value of k is 3\n",
    "To this you need to modify the KMeanc class and add an attribute called cost, for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: \n",
    "- change the random seed to verify that sometimes the algorithm get stuck in a local minima\n",
    "- repeat k-means multiple times to find a good clustering of the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
