{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8852a43e",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "In this lab session we will implement logistic regression to classify gender given the height and weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1359558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sub_sample=True, add_outlier=False):\n",
    "    \"\"\"Load data and convert it to the metric system.\"\"\"\n",
    "    path_dataset = \"Data/height_weight_genders.csv\"\n",
    "    data = np.genfromtxt(\n",
    "        path_dataset, delimiter=\",\", skip_header=1, usecols=[1, 2])\n",
    "    height = data[:, 0]\n",
    "    weight = data[:, 1]\n",
    "    gender = np.genfromtxt(\n",
    "        path_dataset, delimiter=\",\", skip_header=1, usecols=[0],\n",
    "        converters={0: lambda x: 0 if b\"Male\" in x else 1})\n",
    "    # Convert to metric system\n",
    "    height *= 0.025\n",
    "    weight *= 0.454\n",
    "    return height, weight, gender\n",
    "\n",
    "\n",
    "def sample_data(y, x, seed, size_samples):\n",
    "    \"\"\"sample from dataset.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    num_observations = y.shape[0]\n",
    "    random_permuted_indices = np.random.permutation(num_observations)\n",
    "    y = y[random_permuted_indices]\n",
    "    x = x[random_permuted_indices]\n",
    "    return y[:size_samples], x[:size_samples]\n",
    "\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "\n",
    "def de_standardize(x, mean_x, std_x):\n",
    "    \"\"\"Reverse the procedure of standardization.\"\"\"\n",
    "    x = x * std_x\n",
    "    x = x + mean_x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9fa08a",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Load the data, sample 200 datapoints, create the feature matrix X, and standardize it by removing the mean an dividding by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e6115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data.\n",
    "\n",
    "# build sampled x and y.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6349f642",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "define the function `sigmoid(t)` that takes the input and applies the sigmoid function to it <br>\n",
    "$h(t) = \\frac{1}{1+e^{-t}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16573fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ca14ee",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "define the function `calculate_loss(y, tx, theta)` that calculates the logistic loss function\n",
    "$J(\\theta) = -\\frac{1}{N}\\sum_{i=1}^{N} \\left[y^{(i)}log\\left(h_\\theta(x^{(i)})\\right) + \\left(1-y^{(i)}\\right)log\\left(1-h_\\theta(x^{(i)})\\right)\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5047ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, theta):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    \n",
    "    return Lw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e77469",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "define the function `calculate_gradient(y, tx, theta)` that calculates the gradient of the logistic loss <br>\n",
    "$\\frac{\\partial J}{\\partial \\theta_j} = \\frac{1}{N}\\sum_{i=1}^{N} \\left(h_\\theta(x^{(i)}-y^{(i)}\\right)x_{j}^{(i)}$ <br>\n",
    "or in matrix form: <br>\n",
    "$\\nabla_\\theta J = \\frac{1}{N} X^T \\cdot \\left(h\\left(X \\cdot \\theta\\right) - Y\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb8f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, theta):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "\n",
    "    return Gradw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc16fb",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "define the function `learning_by_gradient_descent(y, tx, theta, gamma)` that does one step of gradient descent, return the loss and update the coefficient: <br>\n",
    "$\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial \\theta_j}j(\\theta)$ <br>\n",
    "or in matrix form: <br>\n",
    "$\\theta := \\theta - \\alpha\\nabla_\\theta J$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d05902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, theta, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated theta.\n",
    "    \"\"\"\n",
    "   \n",
    "    return loss, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3041b6c",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "define the function `build_poly(x, degree)` that expands the feature matrix X polynomially according to the degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a311fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"Polynomial expansion of x with the given degree\"\"\"\n",
    "    \n",
    "    return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb4f190",
   "metadata": {},
   "source": [
    "The function `visualization()` is used to visualize the learning progress by plotting the data and the classification borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206449bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(y, x, mean_x, std_x, theta, degree):\n",
    "    \"\"\"visualize the raw data as well as the classification result.\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    # plot raw data\n",
    "    x = de_standardize(x, mean_x, std_x)\n",
    "    males = np.where(y == 0)\n",
    "    females = np.where(y == 1)\n",
    "    # plot raw data with decision boundary\n",
    "    height = np.arange(\n",
    "        np.min(x[:, 0]), np.max(x[:, 0]) + 0.01, step=0.001)\n",
    "    weight = np.arange(\n",
    "        np.min(x[:, 1]), np.max(x[:, 1]) + 0.01, step=0.001)\n",
    "    hx, hy = np.meshgrid(height, weight)\n",
    "    hxy = (np.c_[hx.reshape(-1), hy.reshape(-1)] - mean_x) / std_x\n",
    "    x_temp = build_poly(hxy, degree)\n",
    "    prediction = x_temp.dot(-theta) > 0.0\n",
    "    prediction = prediction.reshape((weight.shape[0], height.shape[0]))\n",
    "    ax.contourf(hx, hy, prediction, 1)\n",
    "    ax.scatter(\n",
    "        x[males, 0], x[males, 1],\n",
    "        marker='.', color=[0.06, 0.06, 1], s=20)\n",
    "    ax.scatter(\n",
    "        x[females, 0], x[females, 1],\n",
    "        marker='*', color=[1, 0.06, 0.06], s=20)\n",
    "    ax.set_xlabel(\"Height\")\n",
    "    ax.set_ylabel(\"Weight\")\n",
    "    ax.set_xlim([min(x[:, 0]), max(x[:, 0])])\n",
    "    ax.set_ylim([min(x[:, 1]), max(x[:, 1])])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3e6acc",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "define the function `logistic_regression_gradient_descent_demo(y, x, pol_degree=1)` that implements logistic regression on the data, and visualizes the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a277a862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, x, pol_degree=1):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = 2.0\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    \n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        \n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "\n",
    "        # converge criterion\n",
    "        \n",
    "    # visualization\n",
    "\n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6dafc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_gradient_descent_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e9c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_gradient_descent_demo(y, x, pol_degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2055f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_gradient_descent_demo(y, x, pol_degree=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98edc36a",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "implement logistic regression with penalized gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df626398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, theta, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated theta.\n",
    "    \"\"\"\n",
    "    \n",
    "    return loss, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1caf318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_gradient_descent_demo(y, x, pol_degree=1):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = 2.0\n",
    "    lambda_ = 1.0\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "  \n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        \n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "            \n",
    "        # converge criterion\n",
    "        \n",
    "    # visualization\n",
    "    \n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94f21ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_penalized_gradient_descent_demo(y, x, pol_degree=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
