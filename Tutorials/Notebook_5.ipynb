{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc606df5",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "In this lab session we will implement logistic regression to classify gender given the height and weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aa1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44593f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sub_sample=True, add_outlier=False):\n",
    "    \"\"\"Load data and convert it to the metric system.\"\"\"\n",
    "    path_dataset = \"Data/height_weight_genders.csv\"\n",
    "    data = np.genfromtxt(\n",
    "        path_dataset, delimiter=\",\", skip_header=1, usecols=[1, 2])\n",
    "    height = np.array(data[:, 0])\n",
    "    weight = np.array(data[:, 1])\n",
    "    gender = np.genfromtxt(\n",
    "        path_dataset, delimiter=\",\", skip_header=1, usecols=[0],\n",
    "        converters={0: lambda x: 0 if \"Male\" in x else 1})\n",
    "    gender = np.array(gender)\n",
    "    # Convert to cm, kg\n",
    "    height *= 2.5\n",
    "    weight *= 0.454\n",
    "    return height, weight, gender\n",
    "\n",
    "\n",
    "def sample_data(x, y, size_samples, seed=0):\n",
    "    \"\"\"sample from dataset.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    num_observations = y.shape[0]\n",
    "    random_permuted_indices = np.random.permutation(num_observations)\n",
    "    x = x[random_permuted_indices]\n",
    "    y = y[random_permuted_indices]\n",
    "    return x[:size_samples], y[:size_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f995a",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Load the data (using the provided function `load_data()`), create the feature matrix `X` and the class vector `C`, and sample 200 datapoints (using the provided function `sample_data()`). The final output should be the 200x3 feature matrix `x` and the corresponding class vector `c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b1213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data.\n",
    "\n",
    "# build matrix X and vector C.\n",
    "\n",
    "# sample 200 datapoints from X and C into x and c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ff0f9",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Plot the sampled data points in the space (height, weight) with two different colors for each gender (use the fucntion `plt.scatter`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d37c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# plt.scatter ...\n",
    "\n",
    "plt.xlabel('Height (cm)')\n",
    "plt.ylabel('Weight (kg)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea647c4",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Define the function `sigmoid(t)` that takes the input and applies the sigmoid function to it <br>\n",
    "$h(t) = \\frac{1}{1+e^{-t}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb83ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350248ec",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Define the function `calculate_loss(x, c, theta)` that calculates the logistic loss function\n",
    "$$J(\\theta) = -\\frac{1}{N}\\sum_{i=1}^{N} \\left[c^{(i)}log\\left(h_\\theta(x^{(i)})\\right) + \\left(1-c^{(i)}\\right)log\\left(1-h_\\theta(x^{(i)})\\right)\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba3c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x, c, theta):\n",
    "    \"\"\"Compute the cost by negative log likelihood.\"\"\"\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73748632",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_loss(x, c, np.array([1, 2, 3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3e9add",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Define the function `calculate_gradient(x, c, theta)` that calculates the gradient of the logistic loss\n",
    "$$\\frac{\\partial J}{\\partial \\theta_j} = \\frac{1}{N}\\sum_{i=1}^{N} \\left(h_\\theta(x^{(i)}-c^{(i)}\\right)x_{j}^{(i)}$$\n",
    "or in matrix form: \n",
    "$$\\nabla_\\theta J = \\frac{1}{N} X^T \\cdot \\left(h\\left(X \\cdot \\theta \\right) - C\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df4106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(x, c, theta):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebf3b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_gradient(x, c, np.array([1, 2, 3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76080433",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Define the function `gradient_descent(x, c, theta, alpha)` with `alpha` the learning rate, which does one step of gradient descent, return the loss and updated `theta`:\n",
    "$$\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial \\theta_j}j(\\theta)$$\n",
    "or in matrix form:\n",
    "$$\\theta := \\theta - \\alpha\\nabla_\\theta J$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af2206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(x, c, theta, alpha):\n",
    "    \"\"\"\n",
    "    Does one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated theta.\n",
    "    \"\"\"\n",
    "    \n",
    "    return loss, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a28391",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "Using the function `learning_by_gradient_descent()`, implement the logistic regression on the data, and visualizes the training process (loss vs. time) to check convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f859f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "# plt.plot(...)\n",
    "\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss J')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc14ed5",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "Plot the line $h_\\theta = 1/2$ on the same plot as the datapoints (see question 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70195b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "# ...\n",
    "\n",
    "plt.xlabel('Height (cm)')\n",
    "plt.ylabel('Weight (kg)')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
