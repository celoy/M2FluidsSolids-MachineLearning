{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f01797a",
   "metadata": {},
   "source": [
    "# Feature Selection/Extraction\n",
    "In this lab session we will use the library scikit-learn to work with the titanic dataset we will apply some feature selection techniques and experiment with dimensionality reduction by using:\n",
    "1. Principal Component Analysis (PCA)\n",
    "2. Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import DecisionBoundaryDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323883d5",
   "metadata": {},
   "source": [
    "We load the titanic data from scikit learn, the `X` data contains information about the passengers on board the titanic, we will be using that data to predict the class `y`, which tells us if a passenger survives the accident or not. \n",
    "The details on the different features name and definition can be found here: https://www.kaggle.com/c/titanic/data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data here by using fetch_openml\n",
    "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace47d30",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Use the functions `head()`, `info()`, `describe()` from the `pandas` library to explore the different features and assess their type (categorical, scalar) and possible missing values (e.g., NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038bd200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3830ac",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Now, we are going to drop the `name` and `ticket` features which are not very informative (although the name may contain a noble or religious title). For that we use the function `drop()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de7ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6ff64c",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Check if any of the remaining features have missing values using `isnull()` and remove those with missing value ratios above 25% using the function `drop()` from `pandas`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e3d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.isnull().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1b4b3",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "There remain features with missing values. Here we decide to drop these data (persons) from the list using the function `notnull()` to find the indices of the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b21ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = X.notnull().all(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb56050",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Check which features are categorical and which are numerical. For the `sex` variable, which is binary, we will simple use 0's and 1's. For the `embarked` variable, we want to code a one-hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e4a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['sex'] = X['sex'].map( {'female': 1, 'male': 0} ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95861e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one hot encoding of column `embarked``\n",
    "one_hot = pd.get_dummies(X['embarked'], dtype=int)\n",
    "\n",
    "# Join one_hot to the existing X\n",
    "\n",
    "# Drop the column `embarked` in X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0bbec3",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Split `X` and `y` into training and testing sets with 20% of the data in the testing set by using `rain_test_split()` from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4860a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a3b2426",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "We will now train a classifier on the training dataset. \n",
    "For that, we will use a pipeline that preprocesses the data and then fits it into a classifier.\n",
    "\n",
    "The idea is to try different classifiers:\n",
    "- `LogisticRegression()`\n",
    "- `SVC(kernel='linear')`\n",
    "- `KNeighborsClassifier(n_neighbors=3)`\n",
    "- `DecisionTreeClassifier()`\n",
    "- `RandomForestClassifier()`\n",
    "\n",
    "Which one give the best results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1fbbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[\n",
    "    ('normalizer', StandardScaler()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b79886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the training set here\n",
    "\n",
    "# Predict the class of the testing dataset\n",
    "y_predict = ...\n",
    "\n",
    "# Assess the quality of the prediction\n",
    "print(\"Number of errors:\", np.sum(y_predict != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b563407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundaries(model, X, ypred, ytrue):                                                                      \n",
    "    _, ax = plt.subplots()\n",
    "    \n",
    "    # Plot decision boundary of the model\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        model,\n",
    "        X[:, :2],\n",
    "        ax=ax,\n",
    "        cmap='cool',\n",
    "        response_method=\"predict\",\n",
    "        plot_method=\"pcolormesh\",\n",
    "        shading=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Plot the testing points\n",
    "    plt.scatter(\n",
    "        X[:, 0],\n",
    "        X[:, 1],\n",
    "        c=ytrue.astype(float),\n",
    "        marker='o',\n",
    "        cmap='jet',\n",
    "    )\n",
    "    \n",
    "    # Plot the predicted points\n",
    "    plt.scatter(\n",
    "        X[ypred != ytrue, 0],\n",
    "        X[ypred != ytrue, 1],\n",
    "        c=ypred[ypred != ytrue].astype(float),\n",
    "        marker='*',\n",
    "        cmap='jet',\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c115e3",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "Redo question 7, but apply PCA to the preprocessed data before fitting it to reduce the dimension of the features, change the number of components used in PCA and check how that affects the testing accuracy. When the number of components is 2, you can use the function `plot_decision_boundaries()` to plot the decision boundary together with the classes predicted and the real classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9de8ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2) \n",
    "pca.fit(X_train)\n",
    "Xnew_train = pca.transform(X_train)\n",
    "Xnew_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c99444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyPython",
   "language": "python",
   "name": "mypython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
