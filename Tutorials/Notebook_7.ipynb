{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7374dc7d",
   "metadata": {},
   "source": [
    "# Feature Selection/Extraction\n",
    "In this lab session we will use the library scikit-learn to work with the titanic dataset we will apply some feature selection techniques and experiment with dimensionality reduction by using:\n",
    "1. Principal Component Analysis (PCA)\n",
    "2. Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9990d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (/Users/Ch/.pyenv/versions/MyPython/lib/python3.10/site-packages/sklearn/metrics/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompose\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnTransformer\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, plot_confusion_matrix\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_openml\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline, make_pipeline\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (/Users/Ch/.pyenv/versions/MyPython/lib/python3.10/site-packages/sklearn/metrics/__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f132330a",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Load the titanic data from scikit learn, the data contains information about the passengers on board the titanic, we will be using that data to predict if a passenger survives the accident or not <br>\n",
    "After loading the dataset split it into training and testing sets with 20% of the data in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c9f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data here by using fetch_openml\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e5c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data here by using train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9c6684",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf046c0",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Check if any of the features have missing values and remove those with high missing value ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6cf57e",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Plot the correlation matrix of the numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b43b20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the correlation matrixk by using sns.heatmap\n",
    "X_comb = pd.concat([X_train, y_train.astype(float)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552865b3",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Combine the parch and sibsp features into one feature called family_size since they are highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793c8feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [X_train, X_test]:\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c58cfd",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Extract the title form the 'name' feature so that we can get some useful information from it.\n",
    "Check all the titles created for males and females and group those that are redundant or rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c96979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the title featrure here\n",
    "for dataset in [X_train, X_test]:\n",
    "    dataset['title'] =  dataset['name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "    dataset.drop([\"name\"], axis=1, inplace=True)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d35ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to group the rare titles\n",
    "X_comb = pd.concat([X_train, X_test])\n",
    "rare_titles = (X_comb['title'].value_counts() < 10)\n",
    "rare_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Group other titles here that you think can be grouped\n",
    "for dataset in [X_train, X_test]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d34253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the ticket feature here\n",
    "for dataset in [X_train, X_test]:\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9133adf",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Check which features are categorical and which are numerical, and define two different transformers to preprocess the data. <br>\n",
    "For categorical features replace the missing values by the most frequent one and onehot encode the categories.<br>\n",
    "For numerical features replace the missing values with the mean and normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fba487",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e92d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the categorical preprocessing here by using Pipeline()\n",
    "cat_cols = ['embarked', 'sex', 'pclass', 'title', 'is_alone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d36eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the numerical preprocessing here by using Pipeline()\n",
    "num_cols = ['age', 'fare', 'family_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6503dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the overall preprocessor by using ColumnTransformer()\n",
    "preprocessor = ColumnTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de79481",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "Define a pipeline that preprocesses the data and then fits it into a random forest classifier.<br>\n",
    "Fit the data and evaluate the data on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d84af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training set here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90574782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the prediction here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804b6220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use classification_report() to evaluate he model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1746548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the confusion matrix by using plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f70369",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "Redo question 7, but apply PCA to the preprocessed data before fitting it to reduce the dimension of the features, change the number of components used in PCA and check how that affects the testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ffa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "modelpca = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('pca', PCA(n_components=n_components)),\n",
    "                      ('classifier', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da6d224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training set here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db57ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the prediction here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b83ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use classification_report() to evaluate he model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d951d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the confusion matrix by using plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23066e74",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "Redo question 7, but apply LDA to the preprocessed data before fitting it to reduce the dimension of the features, and check how that affects the testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cce4328",
   "metadata": {},
   "outputs": [],
   "source": [
    "modellda = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('lda', LinearDiscriminantAnalysis(n_components=1)),\n",
    "                      ('classifier', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d6bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training set here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1dd34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the prediction here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd922a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use classification_report() to evaluate he model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca79d16d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the confusion matrix by using plot_confusion_matrix()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyPython",
   "language": "python",
   "name": "mypython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
