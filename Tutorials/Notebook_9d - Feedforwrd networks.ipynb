{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward networks\n",
    "Notebbok inspired by [Pytorch official tutorials](https://pytorch.org/tutorials/)\n",
    "\n",
    "This sections covers:\n",
    "- simple way of building a feedforward fully connected network\n",
    "- usage of <tt>nn.Sequential</tt> to build a model \n",
    "\n",
    "To begin we want to discuss subclasses of <tt>torch.nn.Module</tt>, which is the PyTorch base class meant to encapsulate behaviors specific to PyTorch Models and their components.\n",
    "\n",
    "One important behavior of torch.nn.Module is registering parameters. If a particular Module subclass has learning weights, these weights are expressed as instances of torch.nn.Parameter. The Parameter class is a subclass of torch.Tensor, with the special behavior that when they are assigned as attributes of a Module, they are added to the list of that modules parameters. These parameters may be accessed through the parameters() method on the Module class.\n",
    "\n",
    "## Perform standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple tiny model\n",
    "As a simple example, here’s a very simple model with two linear layers and an activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the fundamental structure of a PyTorch model: there is an\n",
    "`` __init__()`` method that defines the layers and other components of a\n",
    "model, and a ``forward()`` method where the computation gets done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: create an instance of it and ask it to report on its parameters\n",
    "\n",
    "You can print the model, or any of its submodules, to learn about\n",
    "its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "\n",
      "Just one layer:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0737, -0.0886,  0.0050,  ...,  0.0803,  0.0938, -0.0545],\n",
      "        [ 0.0358,  0.0724,  0.0598,  ...,  0.0843, -0.0485, -0.0427],\n",
      "        [-0.0047,  0.0139,  0.0741,  ...,  0.0540, -0.0214, -0.0120],\n",
      "        ...,\n",
      "        [-0.0673,  0.0208,  0.0386,  ..., -0.0366, -0.0039,  0.0218],\n",
      "        [ 0.0292, -0.0687,  0.0869,  ...,  0.0776, -0.0881,  0.0328],\n",
      "        [-0.0056, -0.0461, -0.0802,  ..., -0.0530, -0.0715,  0.0467]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0258,  0.0102,  0.0764,  0.0950, -0.0138,  0.0628,  0.0230,  0.0205,\n",
      "         0.0703,  0.0857, -0.0120,  0.0430,  0.0621,  0.0646, -0.0558,  0.0848,\n",
      "         0.0053, -0.0637,  0.0266, -0.0329,  0.0012,  0.0965, -0.0024,  0.0922,\n",
      "         0.0313,  0.0272, -0.0703,  0.0661,  0.0631,  0.0045,  0.0341, -0.0491,\n",
      "        -0.0018,  0.0258,  0.0425, -0.0794,  0.0608, -0.0457,  0.0014,  0.0184,\n",
      "        -0.0122, -0.0603, -0.0646,  0.0185, -0.0888, -0.0127, -0.0021, -0.0208,\n",
      "        -0.0641,  0.0179,  0.0126,  0.0045,  0.0126, -0.0112, -0.0883, -0.0013,\n",
      "         0.0341, -0.0382, -0.0916,  0.0693, -0.0380, -0.0229, -0.0560, -0.0915,\n",
      "        -0.0342,  0.0393, -0.0592, -0.0460, -0.0369, -0.0893, -0.0279, -0.0087,\n",
      "        -0.0602,  0.0545, -0.0009,  0.0808, -0.0468,  0.0313, -0.0645,  0.0036,\n",
      "         0.0374, -0.0749,  0.0008,  0.0707,  0.0495,  0.0240, -0.0056,  0.0423,\n",
      "         0.0226, -0.0836,  0.0763, -0.0591,  0.0337,  0.0008,  0.0235, -0.0125,\n",
      "         0.0112, -0.0651, -0.0226, -0.0863, -0.0624,  0.0457,  0.0614, -0.0967,\n",
      "        -0.0951,  0.0695, -0.0460, -0.0157, -0.0809, -0.0955, -0.0040, -0.0692,\n",
      "        -0.0650,  0.0223,  0.0744,  0.0468,  0.0179, -0.0128,  0.0676,  0.0248,\n",
      "        -0.0280, -0.0834, -0.0201,  0.0670, -0.0257,  0.0395,  0.0286, -0.0367,\n",
      "        -0.0178,  0.0375,  0.0021,  0.0962,  0.0321,  0.0908, -0.0586, -0.0054,\n",
      "        -0.0373,  0.0402, -0.0713, -0.0668, -0.0313, -0.0218, -0.0722, -0.0910,\n",
      "        -0.0859, -0.0133,  0.0804,  0.0631,  0.0015,  0.0422,  0.0716,  0.0388,\n",
      "         0.0209,  0.0159,  0.0120, -0.0821, -0.0050,  0.0891,  0.0433,  0.0015,\n",
      "        -0.0185, -0.0950, -0.0541,  0.0999, -0.0271,  0.0489, -0.0095,  0.0977,\n",
      "        -0.0693, -0.0744, -0.0611, -0.0807, -0.0479, -0.0487,  0.0511,  0.0689,\n",
      "        -0.0817, -0.0558,  0.0705, -0.0416, -0.0904,  0.0661, -0.0447, -0.0101,\n",
      "        -0.0392, -0.0594,  0.0107, -0.0619, -0.0103, -0.0623,  0.0790,  0.0161,\n",
      "        -0.0372,  0.0172, -0.0466, -0.0719,  0.0679,  0.0793,  0.0422, -0.0873],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0057,  0.0563, -0.0517,  ...,  0.0001, -0.0610, -0.0002],\n",
      "        [-0.0583,  0.0601,  0.0417,  ...,  0.0327,  0.0658,  0.0170],\n",
      "        [ 0.0540,  0.0664, -0.0367,  ...,  0.0636,  0.0443, -0.0318],\n",
      "        ...,\n",
      "        [ 0.0291,  0.0654, -0.0403,  ...,  0.0522, -0.0079, -0.0444],\n",
      "        [-0.0233,  0.0681,  0.0520,  ..., -0.0646,  0.0369,  0.0141],\n",
      "        [ 0.0459,  0.0150, -0.0377,  ...,  0.0579, -0.0626,  0.0344]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0059, -0.0445,  0.0543,  0.0637, -0.0618, -0.0159, -0.0041,  0.0369,\n",
      "         0.0633,  0.0470], requires_grad=True)\n",
      "\n",
      "\n",
      "Layer params:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0057,  0.0563, -0.0517,  ...,  0.0001, -0.0610, -0.0002],\n",
      "        [-0.0583,  0.0601,  0.0417,  ...,  0.0327,  0.0658,  0.0170],\n",
      "        [ 0.0540,  0.0664, -0.0367,  ...,  0.0636,  0.0443, -0.0318],\n",
      "        ...,\n",
      "        [ 0.0291,  0.0654, -0.0403,  ...,  0.0522, -0.0079, -0.0444],\n",
      "        [-0.0233,  0.0681,  0.0520,  ..., -0.0646,  0.0369,  0.0141],\n",
      "        [ 0.0459,  0.0150, -0.0377,  ...,  0.0579, -0.0626,  0.0344]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0059, -0.0445,  0.0543,  0.0637, -0.0618, -0.0159, -0.0041,  0.0369,\n",
      "         0.0633,  0.0470], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Manipulation Layers and Functions\n",
    "Until now we looked at linear layers. In the next lessons we will study convolutional and recurrent layers.\n",
    "\n",
    "However there are other layer types that perform important functions in models,\n",
    "but don’t participate in the learning process themselves.\n",
    "### Data Manipulation Layers\n",
    "**Normalization layers** re-center and normalize the output of one layer\n",
    "before feeding it to another. Centering the and scaling the intermediate\n",
    "tensors has a number of beneficial effects, such as letting you use\n",
    "higher learning rates without exploding/vanishing gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.rand(1, 4, 4) * 20 + 5\n",
    "print(my_tensor)\n",
    "\n",
    "print(my_tensor.mean())\n",
    "\n",
    "norm_layer = torch.nn.BatchNorm1d(4)\n",
    "normed_tensor = norm_layer(my_tensor)\n",
    "print(normed_tensor)\n",
    "\n",
    "print(normed_tensor.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the cell above, we’ve added a large scaling factor and offset to\n",
    "an input tensor; you should see the input tensor’s ``mean()`` somewhere\n",
    "in the neighborhood of 15. After running it through the normalization\n",
    "layer, you can see that the values are smaller, and grouped around zero\n",
    "- in fact, the mean should be very small (> 1e-8).\n",
    "\n",
    "This is beneficial because many activation functions (discussed below)\n",
    "have their strongest gradients near 0, but sometimes suffer from\n",
    "vanishing or exploding gradients for inputs that drive them far away\n",
    "from zero. Keeping the data centered around the area of steepest\n",
    "gradient will tend to mean faster, better learning and higher feasible\n",
    "learning rates.\n",
    "\n",
    "**Dropout layers** are a tool for encouraging *sparse representations*\n",
    "in your model - that is, pushing it to do inference with less data.\n",
    "\n",
    "Dropout layers work by randomly setting parts of the input tensor\n",
    "*during training* - dropout layers are always turned off for inference.\n",
    "This forces the model to learn against this masked or reduced dataset.\n",
    "For example:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0339, 0.0000, 0.0000, 0.7511],\n",
      "         [1.4009, 0.0000, 1.3929, 0.7077],\n",
      "         [0.0000, 0.0000, 0.5012, 0.3810],\n",
      "         [0.0000, 0.0000, 0.8153, 0.0000]]])\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.4009, 1.6508, 0.0000, 0.0000],\n",
      "         [0.5299, 0.1807, 0.5012, 0.3810],\n",
      "         [0.0000, 1.5821, 0.0000, 0.5558]]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 4, 4)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.4)\n",
    "print(dropout(my_tensor))\n",
    "print(dropout(my_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you can see the effect of dropout on a sample tensor. You can use\n",
    "the optional ``p`` argument to set the probability of an individual\n",
    "weight dropping out; if you don’t it defaults to 0.5.\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "Activation functions make deep learning possible. A neural network is\n",
    "really a program - with many parameters - that *simulates a mathematical\n",
    "function*. If all we did was multiple tensors by layer weights\n",
    "repeatedly, we could only simulate *linear functions;* further, there\n",
    "would be no point to having many layers, as the whole network would\n",
    "reduce could be reduced to a single matrix multiplication. Inserting\n",
    "*non-linear* activation functions between layers is what allows a deep\n",
    "learning model to simulate any function, rather than just linear ones.\n",
    "\n",
    "``torch.nn.Module`` has objects encapsulating all of the major\n",
    "activation functions including ReLU and its many variants, Tanh,\n",
    "Hardtanh, sigmoid, and more. It also includes other functions, such as\n",
    "Softmax, that are most useful at the output stage of a model.\n",
    "\n",
    "### Loss Functions\n",
    "\n",
    "Loss functions tell us how far a model’s prediction is from the correct\n",
    "answer. PyTorch contains a variety of loss functions, including common\n",
    "MSE (mean squared error = L2 norm), Cross Entropy Loss and Negative\n",
    "Likelihood Loss (useful for classifiers), and others.\n",
    "\n",
    "\n",
    "## The sequential module\n",
    "\n",
    "The sequential module is, one of the classes that are used to create the PyTorch neural networks without any explicit class. Basically, the sequential module is a container or we can say that the wrapper class is used to extend the nn modules. \n",
    "\n",
    "In the sequential container, modules will be added to it in the request they be passed in the constructor. \n",
    "\n",
    "Let us look to an example: as usual we define our neural network by subclassing ``nn.Module``, and\n",
    "initialize the neural network layers in ``__init__``. Every ``nn.Module`` subclass implements\n",
    "the operations on input data in the ``forward`` method.\n",
    "\n",
    "Modules are concatenated using the ``nn.Sequential`` container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        output = self.linear_relu_stack(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: use the sequential module to build a feedforward network\n",
    "The network should be as follows:\n",
    "1. It should comprise 10 hidden layers\n",
    "2. Each hidden layer shuold have a different number os units\n",
    "3. For each hidden layer the activation function should be the rectified linear unit function\n",
    "4. The activation of the output layer should be the softmax function (this replaces the sigmoid function when dealing with a multiclass classification problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 10\n",
    "input_size = 1000\n",
    "n_hidden = 10\n",
    "layers_sizes = [int(input_size / 1.5**i) for i in range(n_hidden+1)]\n",
    "mynetwork = NetworkSequential(layers_sizes, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetworkSequential(\n",
      "  (deep_net): Sequential(\n",
      "    (linear1): Linear(in_features=1000, out_features=666, bias=True)\n",
      "    (activation1): ReLU()\n",
      "    (linear2): Linear(in_features=666, out_features=444, bias=True)\n",
      "    (activation2): ReLU()\n",
      "    (linear3): Linear(in_features=444, out_features=296, bias=True)\n",
      "    (activation3): ReLU()\n",
      "    (linear4): Linear(in_features=296, out_features=197, bias=True)\n",
      "    (activation4): ReLU()\n",
      "    (linear5): Linear(in_features=197, out_features=131, bias=True)\n",
      "    (activation5): ReLU()\n",
      "    (linear6): Linear(in_features=131, out_features=87, bias=True)\n",
      "    (activation6): ReLU()\n",
      "    (linear7): Linear(in_features=87, out_features=58, bias=True)\n",
      "    (activation7): ReLU()\n",
      "    (linear8): Linear(in_features=58, out_features=39, bias=True)\n",
      "    (activation8): ReLU()\n",
      "    (linear9): Linear(in_features=39, out_features=26, bias=True)\n",
      "    (activation9): ReLU()\n",
      "    (linear10): Linear(in_features=26, out_features=17, bias=True)\n",
      "    (activation10): ReLU()\n",
      "    (classifier): Linear(in_features=17, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(mynetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
