{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjz3Re-8LODP"
   },
   "source": [
    "# Continuous XOR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will build a simple neural network and train it to classify a dataset representing a continuous XOR (continuous data are generated by introducing some gaussian noise on the binary inputs. Our desired separation of an XOR dataset could look as follows:\n",
    "\n",
    "<center style=\"width: 100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/continuous_xor.svg?raw=1\" width=\"350px\"></center>\n",
    "\n",
    "\n",
    "## Perform standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "08YZnE0RLODV",
    "outputId": "da566b99-dc68-40e2-dddd-bda2b8351cdc"
   },
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "# import os\n",
    "# import math\n",
    "import numpy as np \n",
    "# import time\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the data\n",
    "To generate the data we will use the package `torch.utils.data`, which allows to load the training and test data efficiently.\n",
    "\n",
    "The data package defines two classes which are the standard interface for handling data in PyTorch: `data.Dataset`, and `data.DataLoader`. The dataset class provides an uniform interface to access the training/test data, while the data loader makes sure to efficiently load and stack the data points from the dataset into batches during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0n1lid0LOEM"
   },
   "source": [
    "#### The dataset class\n",
    "\n",
    "The dataset class summarizes the basic functionality of a dataset in a natural way. To define a dataset in PyTorch, we simply specify two functions: `__getitem__`, and `__len__`. The get-item function has to return the $i$-th data point in the dataset, while the len function returns the size of the dataset. For the XOR dataset, we can define the dataset class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PPmj1FAcLOEM"
   },
   "outputs": [],
   "source": [
    "class XORDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, size, std=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            size - Number of data points we want to generate\n",
    "            std - Standard deviation of the noise (see generate_continuous_xor function)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.std = std\n",
    "        self.generate_continuous_xor()\n",
    "\n",
    "    def generate_continuous_xor(self):\n",
    "        # Each data point in the XOR dataset has two variables, x and y, that can be either 0 or 1\n",
    "        # The label is their XOR combination, i.e. 1 if only x or only y is 1 while the other is 0.\n",
    "        # If x=y, the label is 0.\n",
    "        data = torch.randint(low=0, high=2, size=(self.size, 2), dtype=torch.float32)\n",
    "        label = (data.sum(dim=1) == 1).to(torch.long)\n",
    "        # To make it slightly more challenging, we add a bit of gaussian noise to the data points.\n",
    "        data += self.std * torch.randn(data.shape)\n",
    "\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the idx-th data point of the dataset\n",
    "        # If we have multiple things to return (data point and label), we can return them as tuple\n",
    "        data_point = self.data[idx]\n",
    "        data_label = self.label[idx]\n",
    "        return data_point, data_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWqg4ErULOEN"
   },
   "source": [
    "Let's try to create such a dataset and inspect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RAb_jZTCLOEN",
    "outputId": "4fdf5ab8-e3aa-4dec-c196-d1f5f7666ccd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 200\n",
      "Data point 0: (tensor([1.0160, 0.0804]), tensor(1))\n"
     ]
    }
   ],
   "source": [
    "dataset = XORDataset(size=200)\n",
    "print(\"Size of dataset:\", len(dataset))\n",
    "print(\"Data point 0:\", dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--w78DZBLOEO"
   },
   "source": [
    "### EXERCISE:  plot the samples below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2F4q_eUBLOEO"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Remember to do data.cpu().numpy(), etc before plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lu-Ixf7tLOEP"
   },
   "source": [
    "### QUICK RECALL: The data loader class\n",
    "\n",
    "The class `torch.utils.data.DataLoader` represents a Python iterable over a dataset with support for automatic batching, multi-process data loading and many more features. The data loader communicates with the dataset using the function `__getitem__`, and stacks its outputs as tensors over the first dimension to form a batch.\n",
    "In contrast to the dataset class, we usually don't have to define our own data loader class, but can just create an object of it with the dataset as input. Additionally, we can configure our data loader with the following input arguments (only a selection, see full list [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)):\n",
    "\n",
    "* `batch_size`: Number of samples to stack per batch\n",
    "* `shuffle`: If True, the data is returned in a random order. This is important during training for introducing stochasticity. \n",
    "* `num_workers`: Number of subprocesses to use for data loading. The default, 0, means that the data will be loaded in the main process which can slow down training for datasets where loading a data point takes a considerable amount of time (e.g. large images). More workers are recommended for those, but can cause issues on Windows computers. For tiny datasets as ours, 0 workers are usually faster.\n",
    "* `pin_memory`: If True, the data loader will copy Tensors into CUDA pinned memory before returning them. This can save some time for large data points on GPUs. Usually a good practice to use for a training set, but not necessarily for validation and test to save memory on the GPU.\n",
    "* `drop_last`: If True, the last batch is dropped in case it is smaller than the specified batch size. This occurs when the dataset size is not a multiple of the batch size. Only potentially helpful during training to keep a consistent batch size.\n",
    "\n",
    "### EXERCISE: create a data loader for the training test and one for the test set\n",
    "\n",
    "You can create train and test sets separately or create a single dataset and split them using sklearn function. \n",
    "Do not forget to import it using `from sklearn.model_selection import train_test_split`.\n",
    "\n",
    "The training set should be 2500 points and batch size 128. Use 500 and batch size 25 for the test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WvlWAHQ0LOEQ"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DOMbsI0lLOEQ",
    "outputId": "749e0074-ba8c-4a85-e5c8-bcfd2b95d4e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inputs torch.Size([8, 2]) \n",
      " tensor([[ 1.1871,  0.1322],\n",
      "        [ 1.0012,  0.0388],\n",
      "        [ 0.1054, -0.0954],\n",
      "        [-0.0770,  1.0230],\n",
      "        [-0.0293, -0.0560],\n",
      "        [-0.0918, -0.0035],\n",
      "        [ 0.9766,  0.0374],\n",
      "        [ 1.1661, -0.1935]])\n",
      "Data labels torch.Size([8]) \n",
      " tensor([1, 1, 0, 1, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# next(iter(...)) catches the first batch of the data loader\n",
    "# If shuffle is True, this will return a different batch every time we run this cell\n",
    "# For iterating over the whole dataset, we can simple use \"for batch in data_loader: ...\"\n",
    "\n",
    "data_loader = data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "data_inputs, data_labels = next(iter(data_loader))\n",
    "\n",
    "# The shape of the outputs are [batch_size, d_1,...,d_N] where d_1,...,d_N are the \n",
    "# dimensions of the data point returned from the dataset class\n",
    "print(\"Data inputs\", data_inputs.shape, \"\\n\", data_inputs)\n",
    "print(\"Data labels\", data_labels.shape, \"\\n\", data_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dboum4tsLOEG"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "### EXERCISE:\n",
    "* Construct the class for minimal network with a input layer, one hidden layer with tanh as activation function, and an output layer. \n",
    "* Call an istance of the model using four hidden neurons.\n",
    "* Print the model and its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Rk8rdeMfLOEG"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "After defining the model and the dataset, it is time to prepare the optimization of the model. \n",
    "\n",
    "Recall that during training, we will perform the following steps:\n",
    "\n",
    "1. Get a batch from the data loader\n",
    "2. Obtain the predictions from the model for the batch\n",
    "3. Calculate the loss based on the difference between predictions and labels\n",
    "4. Backpropagation: calculate the gradients for every parameter with respect to the loss\n",
    "5. Update the parameters of the model in the direction of the gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6hG89kALOER"
   },
   "source": [
    "#### Loss modules\n",
    "\n",
    "We can calculate the loss for a batch by simply performing a few tensor operations as those are automatically added to the computation graph. For instance, for binary classification, we can use Binary Cross Entropy (BCE) which is defined as follows:\n",
    "\n",
    "$$\\mathcal{L}_{BCE} = -\\sum_i \\left[ y_i \\log x_i + (1 - y_i) \\log (1 - x_i) \\right]$$\n",
    "\n",
    "where $y$ are our labels, and $x$ our predictions, both in the range of $[0,1]$. However, PyTorch already provides a list of predefined loss functions which we can use (see [here](https://pytorch.org/docs/stable/nn.html#loss-functions) for a full list). For instance, for BCE, PyTorch has two modules: `nn.BCELoss()`, `nn.BCEWithLogitsLoss()`. While `nn.BCELoss` expects the inputs $x$ to be in the range $[0,1]$, i.e. the output of a sigmoid, `nn.BCEWithLogitsLoss` combines a sigmoid layer and the BCE loss in a single class. This version is numerically more stable than using a plain Sigmoid followed by a BCE loss because of the logarithms applied in the loss function. Hence, it is adviced to use loss functions applied on \"logits\" where possible (remember to not apply a sigmoid on the output of the model in this case!). For our model defined above, we therefore use the module `nn.BCEWithLogitsLoss`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOADAZKsLOER"
   },
   "source": [
    "#### Stochastic Gradient Descent\n",
    "\n",
    "For updating the parameters, you will use the `torch.optim.SGD` as seen in the prevous sections.\n",
    "\n",
    "Remember that input to the optimizer are the parameters of the model: `model.parameters()`.\n",
    "\n",
    "\n",
    "A good default value of the learning rate for a small network as ours is 0.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Train your network and evaluate it on the test data\n",
    "\n",
    "* Train the network for 100 epochs\n",
    "* Evaluate it simultaneously (do not forget to deactivate gradients using `with torch.no_grad()`).\n",
    "* Use the accuracy as metric for evaluation. You should also plot the loss function for training and test data set accross epochs\n",
    "\n",
    "Before starting it is better to redo the following steps:\n",
    "* Create a train set of 2500 points and batch size 128. Use 500 and batch size 25 for the test\n",
    "* Creat a new istance of your model and send it to the device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YiufBhX6LOES"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8yr5XaVLOES"
   },
   "source": [
    "### Training\n",
    "\n",
    "Finally, we are ready to train our model. As a first step, we create a slightly larger dataset and specify a data loader with a larger batch size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwwK_Z7ILOET"
   },
   "source": [
    "HINT: \n",
    "\n",
    "1. you can write a small training function. Remember our five steps: load a batch, obtain the predictions, calculate the loss, backpropagate, and update. Here the model should be in training mode. This is done by calling `model.train()`. The training mode is needed to correcly implement BatchNorm and Dropuot.\n",
    "\n",
    "2. Similarly you can write a evaluate function and call it in the training function. Here the model should be in evaluation mode (use `model.eval()`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE, code for evaluation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "r4cqPRQELOEW"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE, code for training function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "eb0f1646f5f4459c86c22f0845150144"
     ]
    },
    "id": "7AvGN0Z0LOEX",
    "outputId": "48361c61-ce1d-4010-aeba-d21184203ca9"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE, here you call the training function and get the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: plot the losses and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlsfavbQLOEd"
   },
   "source": [
    "If we trained our model correctly, we should see a score close to 100% accuracy. However, this is only possible because of our simple task, and unfortunately, we usually don't get such high scores on test sets of more complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: save the model and reload it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BsZNlhjrLOEZ",
    "outputId": "0d797c01-df1c-4d96-f528-7ce5c53e9eed"
   },
   "outputs": [],
   "source": [
    "# SAVE THE MODEL\n",
    "\n",
    "\n",
    "# A detailed tutorial on saving and loading models in PyTorch \n",
    "# can be found [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrmmA_9vLOEd"
   },
   "source": [
    "### EXERCISE: visualize the classification boundaries using the model just load\n",
    "\n",
    "To visualize what our model has learned, we can perform a prediction for every data point in a range of $[-0.5, 1.5]$, and visualize the predicted class as in the sample figure at the beginning of this section. This shows where the model has created decision boundaries, and which points would be classified as $0$, and which as $1$. We therefore get a background image out of blue (class 0) and orange (class 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LCABoFJeLOEe",
    "outputId": "cd73c4c0-fdad-4086-c683-33c7c3697229"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# @torch.no_grad() # Decorator, same effect as \"with torch.no_grad(): ...\" over the whole function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
