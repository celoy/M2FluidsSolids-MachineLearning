{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93579ba4",
   "metadata": {},
   "source": [
    "In this notebook, we will implement a simple neural network from scratch using only Python and NumPy.\n",
    "This will help you understand the fundamental concepts behind neural networks, including:\n",
    "- Forward propagation\n",
    "- Backward propagation\n",
    "- Gradient descent\n",
    "\n",
    "First of all, let us import the essential modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dae432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba745f49",
   "metadata": {},
   "source": [
    "Let us start with a neural network used for binary classification.\n",
    "For this, we create a synthetic dataset with two classes. We use matplotlib to plot the dataset in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simple dataset for binary classification\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(400, 2)  # 300 data points with 2 features\n",
    "y = (X[:, 0]**2 + X[:, 1]**2 < 1).astype(int)  # Circular decision boundary\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Plot the dataset\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y.flatten(), cmap=\"viridis\")\n",
    "plt.title(\"Synthetic Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c11740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset: 80% for training, 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shape of the resulting datasets\n",
    "print(f\"Training set: X_train shape = {X_train.shape}, y_train shape = {y_train.shape}\")\n",
    "print(f\"Testing set: X_test shape = {X_test.shape}, y_test shape = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a38218b",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "How many inputs does our neural network require?\n",
    "How many outputs does our neural network provide?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cdf7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2    # Number of features\n",
    "output_dim = 1   # Number of output units (binary classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d380c44d",
   "metadata": {},
   "source": [
    "We build a neural network with a given number of hidden layers. For the moment, we will use a single hidden layer with four neurons. Later, we will work on the impact of this number of the training.\n",
    "### Question 2\n",
    "Fill in the cell below with the missing information, using a list of integers to provide the architecture of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized architecture setup\n",
    "layer_dims = [input_dim, 4, output_dim]  # Example: Input layer (2), 2 hidden layers (4 and 3 units), and output layer (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6552404d",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Before training the model, we need to initialize the weights and biases randomly. Fill in the cell below with the missing information to initialize the weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and biases\n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Initialize parameters for a neural network with multiple layers.\n",
    "    Arguments:\n",
    "    - layer_dims: List containing the dimensions of each layer.\n",
    "\n",
    "    Returns:\n",
    "    - parameters: Dictionary containing initialized weights and biases for all layers.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    parameters = {}\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        parameters[f\"W{l}\"] = np.random.randn(layer_dims[l-1], layer_dims[l]) * 0.01\n",
    "        parameters[f\"b{l}\"] = np.zeros((1, layer_dims[l]))\n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters(layer_dims)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b31611",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Write a Python function for a sigmoid activation function and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f9836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf17b95",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Write a Python function for a ReLU activation function and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca576bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation function\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "# Derivative of the ReLU function\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c215fe5",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Now we need to write a function to evaluate the neural network. This is the so-called forward pass. Fill in the cell below with the missing information so that each neuron produces as output W.x+b, with x the output of the previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9680751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Forward propagate through an arbitrary number of layers.\n",
    "    Arguments:\n",
    "    - X: Input data\n",
    "    - parameters: Dictionary containing weights and biases for all layers.\n",
    "\n",
    "    Returns:\n",
    "    - activations: Dictionary of activations for all layers.\n",
    "    \"\"\"\n",
    "    activations = {'A0': X}  # Input layer activation\n",
    "    L = len(parameters) // 2  # Number of layers (parameters include W1, b1, ..., WL, bL)\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        Z = np.dot(activations[f\"A{l-1}\"], parameters[f\"W{l}\"]) + parameters[f\"b{l}\"]\n",
    "        if l == L:  # Output layer\n",
    "            activations[f\"A{l}\"] = sigmoid(Z)\n",
    "        else:  # Hidden layers\n",
    "            activations[f\"A{l}\"] = relu(Z)\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afdd7ab",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "The most important part of training a model is the backward pass, by which the parameters are updated using the gradient and the learning rate. Fill in the cell below with the missing information to update the parameters following a gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc09fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(activations, y, parameters, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Perform backward propagation for an arbitrary number of layers.\n",
    "    Arguments:\n",
    "    - activations: Dictionary of activations for all layers.\n",
    "    - y: True labels.\n",
    "    - parameters: Dictionary containing weights and biases.\n",
    "    - learning_rate: Learning rate for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "    - Updated parameters.\n",
    "    \"\"\"\n",
    "    m = y.shape[0]  # Number of training examples\n",
    "    gradients = {}\n",
    "    L = len(parameters) // 2  # Number of layers\n",
    "\n",
    "    # Output layer gradients\n",
    "    dZ = activations[f\"A{L}\"] - y\n",
    "    gradients[f\"dW{L}\"] = np.dot(activations[f\"A{L-1}\"].T, dZ) / m\n",
    "    gradients[f\"db{L}\"] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "\n",
    "    # Backpropagate through hidden layers\n",
    "    for l in range(L-1, 0, -1):\n",
    "        dA = np.dot(dZ, parameters[f\"W{l+1}\"].T)\n",
    "        dZ = dA * relu_derivative(activations[f\"A{l}\"])\n",
    "        gradients[f\"dW{l}\"] = np.dot(activations[f\"A{l-1}\"].T, dZ) / m\n",
    "        gradients[f\"db{l}\"] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "\n",
    "    # Update parameters\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[f\"W{l}\"] -= learning_rate * gradients[f\"dW{l}\"]\n",
    "        parameters[f\"b{l}\"] -= learning_rate * gradients[f\"db{l}\"]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42bf0bf",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "Once the forward and backward passes are correctly implemented. The idea is to train the model iteratively. Each iteration is called epoch. Fill in the cell below with the missing information. Choose in particular a number of epochs and a value for the learning rate. Select different number of epochs and learning rate and describe what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb82e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example: Training the generalized network\n",
    "epochs = 30000\n",
    "learning_rate = 0.1\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward propagation on training data\n",
    "    activations = forward_propagation(X_train, parameters)\n",
    "    A_train_final = activations[f\"A{len(layer_dims)-1}\"]  # Output layer activation on training data\n",
    "    \n",
    "    # Compute training loss\n",
    "    train_loss = -np.mean(y_train * np.log(A_train_final) + (1 - y_train) * np.log(1 - A_train_final))\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Forward propagation on validation data\n",
    "    val_activations = forward_propagation(X_test, parameters)\n",
    "    A_val_final = val_activations[f\"A{len(layer_dims)-1}\"]  # Output layer activation on validation data\n",
    "    \n",
    "    # Compute validation loss\n",
    "    val_loss = -np.mean(y_test * np.log(A_val_final) + (1 - y_test) * np.log(1 - A_val_final))\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Backward propagation on training data\n",
    "    parameters = backward_propagation(activations, y_train, parameters, learning_rate)\n",
    "    \n",
    "    # Print training and validation loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Training Loss: {train_loss}, Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d9a53d",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "The best way to assess the performance of the training and the model is by plotting as a function of the epochs the training and the validation losses. Write the Python lines in the cell below to plot these two curves for different values of epochs and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3fbc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdccbb8",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "Perform several training, increasing the complexity of the model. You can do that by increasing the number of neurons in each hidden layer and/or the number of hidden layers. Plot the training/validation losses as a function of the number of epochs. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e424101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [input_dim, 10, 10, output_dim]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "epochs = 30000\n",
    "learning_rate = 0.1\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward propagation on training data\n",
    "    activations = forward_propagation(X_train, parameters)\n",
    "    A_train_final = activations[f\"A{len(layer_dims)-1}\"]  # Output layer activation on training data\n",
    "    \n",
    "    # Compute training loss\n",
    "    train_loss = -np.mean(y_train * np.log(A_train_final) + (1 - y_train) * np.log(1 - A_train_final))\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Forward propagation on validation data\n",
    "    val_activations = forward_propagation(X_test, parameters)\n",
    "    A_val_final = val_activations[f\"A{len(layer_dims)-1}\"]  # Output layer activation on validation data\n",
    "    \n",
    "    # Compute validation loss\n",
    "    val_loss = -np.mean(y_test * np.log(A_val_final) + (1 - y_test) * np.log(1 - A_val_final))\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Backward propagation on training data\n",
    "    parameters = backward_propagation(activations, y_train, parameters, learning_rate)\n",
    "    \n",
    "    # Print training and validation loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Training Loss: {train_loss}, Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5a584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d595d5fc",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "Now we will use different trained models and will predict the decision boundary. For this, we will first write a function that plots the decision boundary for a given model. Fill in the cell below with the missing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d1f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(forward_propagation, X, y, parameters, layer_dims):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary for a given model.\n",
    "\n",
    "    Arguments:\n",
    "    - forward_propagation: Function to perform forward propagation.\n",
    "    - X: Input data, used to define the feature space for plotting.\n",
    "    - y: True labels, used for scatter plot coloring.\n",
    "    - parameters: Dictionary of weights and biases for the neural network.\n",
    "    - layer_dims: List defining the number of units in each layer.\n",
    "\n",
    "    Returns:\n",
    "    - None (displays the decision boundary plot).\n",
    "    \"\"\"\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    # Generate a grid of points to evaluate the model\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, 0.01),\n",
    "        np.arange(y_min, y_max, 0.01)\n",
    "    )\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]  # Flatten and combine the meshgrid into input data shape\n",
    "    \n",
    "    # Predict for each point on the grid\n",
    "    activations = forward_propagation(grid, parameters)\n",
    "    predictions = activations[f\"A{len(layer_dims) - 1}\"]  # Final output layer activations\n",
    "    predictions = (predictions > 0.5).astype(int)  # Convert probabilities to binary labels\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    plt.contourf(xx, yy, predictions.reshape(xx.shape), alpha=0.7, cmap=\"viridis\")\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y.flatten(), edgecolors=\"k\", cmap=\"viridis\")\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.show()\n",
    "\n",
    "# Example call\n",
    "plot_decision_boundary(forward_propagation, X, y, parameters, layer_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496ba89",
   "metadata": {},
   "source": [
    "The example we have considered is very simple and one would like to apply this to more complex boundaries. Let us therefore consider the examples of the moons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f1eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset with two interleaving moon shapes\n",
    "X, y = make_moons(n_samples=1000, noise=0.15, random_state=0)\n",
    "y = y.reshape(-1, 1)  # Reshape to match the required format\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y.flatten(), cmap=\"viridis\")\n",
    "plt.title(\"Dataset: Moons with Noise\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab16ce66",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "Perform the previous steps for this dataset. Compare the performance of the same model trained and evaluated with the synthetic dataset that we generated at the beginning and with the moon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3628536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset: 80% for training, 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shape of the resulting datasets\n",
    "print(f\"Training set: X_train shape = {X_train.shape}, y_train shape = {y_train.shape}\")\n",
    "print(f\"Testing set: X_test shape = {X_test.shape}, y_test shape = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba07385",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [input_dim, 20, 10, output_dim]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "epochs = 30000\n",
    "learning_rate = 0.1\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward propagation on training data\n",
    "    activations = forward_propagation(X_train, parameters)\n",
    "    A_train_final = activations[f\"A{len(layer_dims)-1}\"]  # Output layer activation on training data\n",
    "    \n",
    "    # Compute training loss\n",
    "    train_loss = -np.mean(y_train * np.log(A_train_final) + (1 - y_train) * np.log(1 - A_train_final))\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Forward propagation on validation data\n",
    "    val_activations = forward_propagation(X_test, parameters)\n",
    "    A_val_final = val_activations[f\"A{len(layer_dims)-1}\"]  # Output layer activation on validation data\n",
    "    \n",
    "    # Compute validation loss\n",
    "    val_loss = -np.mean(y_test * np.log(A_val_final) + (1 - y_test) * np.log(1 - A_val_final))\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Backward propagation on training data\n",
    "    parameters = backward_propagation(activations, y_train, parameters, learning_rate)\n",
    "    \n",
    "    # Print training and validation loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Training Loss: {train_loss}, Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87547240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(forward_propagation, X, y, parameters, layer_dims)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
