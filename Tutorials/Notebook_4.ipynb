{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33098c87",
   "metadata": {},
   "source": [
    "# Testing and Comparing Different Optimizers\n",
    "\n",
    "In this lab session we will implement some gradient descent algorithms and compare their performance\n",
    "Optimizers: Vanilla Gradient Descent, Momentum, NAG, Adagrad, RMSprop, Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85c34a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e20817",
   "metadata": {},
   "source": [
    "The function to minimize is the **Beale's function**: <br>\n",
    "$f(x) = (1.5-x_1+x_1x_2)^2 + (2.25-x_1+x_1{x_2}^2)^2 + (2.625-x_1+x_1{x_2}^3)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87462f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f  = lambda x, y: (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67806359",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Analytically compute the gradient of f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea87fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradf_x = lambda x, y:\n",
    "# gradf_y = lambda x, y: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a479d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax, xstep = -4.5, 4.5, .2\n",
    "ymin, ymax, ystep = -4.5, 4.5, .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff55b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591eaa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = f(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d064d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "minima = np.array([3., .5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d1013",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(*minima)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9dccba",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Create a surface plot to visualize the function and mark the global minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708589f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = plt.axes(projection='3d', elev=50, azim=-50)\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.set_zlabel('$z$')\n",
    "\n",
    "ax.set_xlim((xmin, xmax))\n",
    "ax.set_ylim((ymin, ymax))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa52e9",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Create a contour plot and a quiver plot of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e285c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz_dx = gradf_x(x, y)\n",
    "dz_dy = gradf_y(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5c3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "ax.set_xlim((xmin, xmax))\n",
    "ax.set_ylim((ymin, ymax))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01700aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba33602",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "This vanilla gradient descent: <br>\n",
    "$\\theta_t = \\theta_{t-1} - \\alpha\\nabla_\\theta f(\\theta_{t-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b26c22",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Implement vanilla gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121dc2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "x0 = np.array([1.1, 2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0f7777",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gd = np.zeros((x0.shape[0], steps))\n",
    "iteration = []\n",
    "losses_gd = []\n",
    "loss = f(*x0)\n",
    "print(f'X:{x0}, f:{loss}')\n",
    "for i in range(steps):\n",
    "    print(f'X:{x0}, f:{loss}, i:{i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a9e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Loss(iteration, losses):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))    \n",
    "    ax.semilogy(iteration, losses)\n",
    "    ax.set_xlabel('$i$')\n",
    "    ax.set_ylabel('$loss$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba3a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Trajectory(x_val, label):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax.contour(x, y, z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)\n",
    "    ax.plot(x_val[0,:], x_val[1,:], 'b', label=label, lw=2)\n",
    "    ax.plot(*minima, 'r*', markersize=18)\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "\n",
    "    ax.set_xlim((xmin, xmax))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3979ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_Loss(iteration, losses_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbdd308",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_Trajectory(x_gd, 'GD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e1a0d3",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "GD with momentum helps accelerate GD and prevents it from being stuck in regions with small gradients: <br>\n",
    "$v_t = \\gamma v_{t-1} + \\alpha \\nabla_\\theta f(\\theta_{t-1})$ <br>\n",
    "$\\theta_t = \\theta_{t-1} - v_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375d717b",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Implement GD with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887766be",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "gamma = 0.9\n",
    "x0 = np.array([1.1, 2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb960acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = []\n",
    "losses_mom = []\n",
    "loss = f(*x0)\n",
    "print(f'X:{x0}, f:{loss}')\n",
    "x_mom = np.zeros((x0.shape[0], steps))\n",
    "for i in range(steps):\n",
    "\n",
    "    print(f'X:{x0}, f:{loss}, i:{i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5454ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_Loss(iteration, losses_mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a0714",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_Trajectory(x_mom, 'momentum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735c222",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient (NAG)\n",
    "NAG is very similar to momentum but it calculates the gradient at the next position to stabilize the learning process. <br>\n",
    "$v_t = \\gamma v_{t-1} + \\alpha \\nabla_\\theta f(\\theta_{t-1} - \\gamma v_{t-1})$ <br>\n",
    "$\\theta_t = \\theta_{t-1} - v_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75034492",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Implement NAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4a1cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "gamma = 0.9\n",
    "x0 = np.array([1.1, 2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = []\n",
    "losses_nag = []\n",
    "loss = f(*x0)\n",
    "print(f'X:{x0}, f:{loss}')\n",
    "x_nag = np.zeros((x0.shape[0], steps))\n",
    "for i in range(steps):\n",
    "\n",
    "    print(f'X:{x0}, f:{loss}, i:{i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_Loss(iteration, losses_nag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029b5ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_Trajectory(x_nag, 'NAG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b381b003",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    "Adagrad adapts the learning rate based on the frequency of previous parameter updates. <br>\n",
    "more updates --> smaller learning rate <br>\n",
    "less updates --> higher learning rate <br>\n",
    "$G_t = G_{t-1} + \\left(\\nabla_\\theta f(\\theta_{t-1})\\right)^2$ <br>\n",
    "$\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{G_t + \\epsilon}}\\nabla_\\theta f(\\theta_{t-1})$ <br>\n",
    "with $G_0 = \\vec{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04afed6",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "Implement Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad9aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "x0 = np.array([1.1, 2.])\n",
    "Eg = np.zeros(x0.shape)\n",
    "epsilon = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3375f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_adag = np.zeros((x0.shape[0], steps))\n",
    "iteration = []\n",
    "losses_adag = []\n",
    "loss = f(*x0)\n",
    "print(f'X:{x0}, f:{loss}')\n",
    "for i in range(steps):\n",
    "\n",
    "    print(f'X:{x0}, f:{loss}, i:{i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73334c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_Loss(iteration, losses_adag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2da88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_Trajectory(x_adag, 'Adagrad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ddaddd",
   "metadata": {},
   "source": [
    "## RMSprop\n",
    "RMSprop is very similar to Adagrad but it keeps the average over a finite window resolving the radically diminishing leanring rates. <br>\n",
    "$G_t = 0.9G_{t-1} + 0.1\\left(\\nabla_\\theta f(\\theta_{t-1})\\right)^2$ <br>\n",
    "$\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{G_t + \\epsilon}}\\nabla_\\theta f(\\theta_{t-1})$ <br>\n",
    "with $G_0 = \\vec{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffad68e0",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "Implement RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97560378",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "x0 = np.array([1.1, 2.])\n",
    "Eg = np.zeros(x0.shape)\n",
    "epsilon = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbab7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rms = np.zeros((x0.shape[0], steps))\n",
    "iteration = []\n",
    "losses_rms = []\n",
    "loss = f(*x0)\n",
    "print(f'X:{x0}, f:{loss}')\n",
    "for i in range(steps):\n",
    "    \n",
    "    print(f'X:{x0}, f:{loss}, i:{i}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685944df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_Loss(iteration, losses_rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176cf58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_Trajectory(x_rms, 'RMSprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c63bc3",
   "metadata": {},
   "source": [
    "## Adam\n",
    "Adam is an optimizer that combines momentum GD and learning rate adaptation. <br>\n",
    "$m_t = \\beta_1 m_{t-1} + (1-\\beta1)\\nabla_\\theta f(\\theta_{t-1})$ <br>\n",
    "$v_t = \\beta_2 v_{t-1} + (1-\\beta2)\\left(\\nabla_\\theta f(\\theta_{t-1})\\right)^2$ <br>\n",
    "$\\hat{m}_t = \\frac{m_t}{1-{\\beta_1}^t}$ <br>\n",
    "$\\hat{v}_t = \\frac{v_t}{1-{\\beta_2}^t}$ <br>\n",
    "$\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{\\hat{v}_t + \\epsilon}}\\hat{m}_t$ <br>\n",
    "with $m_0 = v_0 = \\vec{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff6a922",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "Implement Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae9598",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "epsilon = 1e-8\n",
    "beta1 = 0.9\n",
    "beta2 = 0.9\n",
    "x0 = np.array([1.1, 2.])\n",
    "mt = np.array([0., 0.])\n",
    "vt = np.array([0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = []\n",
    "losses_adam = []\n",
    "loss = f(*x0)\n",
    "print(f'X:{x0}, f:{loss}')\n",
    "x_adam = np.zeros((x0.shape[0], steps))\n",
    "for i in range(steps):\n",
    "\n",
    "    print(f'X:{x0}, f:{loss}, i:{i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_Loss(iteration, losses_adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6dfc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_Trajectory(x_adam, 'Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2097e010",
   "metadata": {},
   "source": [
    "## Simulate Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d3ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryAnimation(animation.FuncAnimation):\n",
    "    \n",
    "    def __init__(self, *paths, labels=[], fig=None, ax=None, frames=None, \n",
    "                 interval=60, repeat_delay=5, blit=True, **kwargs):\n",
    "\n",
    "        if fig is None:\n",
    "            if ax is None:\n",
    "                fig, ax = plt.subplots()\n",
    "            else:\n",
    "                fig = ax.get_figure()\n",
    "        else:\n",
    "            if ax is None:\n",
    "                ax = fig.gca()\n",
    "\n",
    "        self.fig = fig\n",
    "        self.ax = ax\n",
    "        \n",
    "        self.paths = paths\n",
    "\n",
    "        if frames is None:\n",
    "            frames = max(path.shape[1] for path in paths)\n",
    "  \n",
    "        self.lines = [ax.plot([], [], label=label, lw=2)[0] \n",
    "                      for _, label in zip_longest(paths, labels)]\n",
    "        self.points = [ax.plot([], [], 'o', color=line.get_color())[0] \n",
    "                       for line in self.lines]\n",
    "\n",
    "        super(TrajectoryAnimation, self).__init__(fig, self.animate, init_func=self.init_anim,\n",
    "                                                  frames=frames, interval=interval, blit=blit,\n",
    "                                                  repeat_delay=repeat_delay, **kwargs)\n",
    "\n",
    "    def init_anim(self):\n",
    "        for line, point in zip(self.lines, self.points):\n",
    "            line.set_data([], [])\n",
    "            point.set_data([], [])\n",
    "        return self.lines + self.points\n",
    "\n",
    "    def animate(self, i):\n",
    "        for line, point, path in zip(self.lines, self.points, self.paths):\n",
    "            line.set_data(*path[::,:i])\n",
    "            point.set_data(*path[::,i-1:i])\n",
    "        return self.lines + self.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Losses(iteration, labels, losses):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    for loss in losses:\n",
    "        ax.semilogy(iteration, loss)\n",
    "    ax.legend(labels)\n",
    "    ax.set_xlabel('i')\n",
    "    ax.set_ylabel('loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c53a200",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "Use the function `TrajectoryAnimation()` to make an animation that shows the learning process of each method and comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838a95f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"GD\", \"Momentum\", \"NAG\", \"Adagrad\", \"RMSprop\", \"Adam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b907d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [x_gd[:,0:-1:10], x_mom[:,0:-1:10], x_nag[:,0:-1:10],\n",
    "         x_adag[:,0:-1:10], x_rms[:,0:-1:10], x_adam[:,0:-1:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3c4e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [losses_gd, losses_mom, losses_nag, losses_adag, losses_rms, losses_adam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1b29bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_Losses(iteration, methods, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f390e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.contour(x, y, z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)\n",
    "ax.plot(*minima, 'r*', markersize=10)\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "ax.set_xlim((xmin, xmax))\n",
    "ax.set_ylim((ymin, ymax))\n",
    "\n",
    "anim = TrajectoryAnimation(*paths, labels=methods, ax=ax)\n",
    "\n",
    "ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c76572",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
