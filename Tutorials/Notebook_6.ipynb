{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb9e615",
   "metadata": {},
   "source": [
    "# Classifiers\n",
    "In this lab session we will use the library scikit-learn to apply the following classifier algorithms:\n",
    "1. Support Vector Machines (SVM)\n",
    "2. K-Nearest Neighbor\n",
    "3. Naive Bayes\n",
    "4. Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187a5721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, svm, tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.inspection import DecisionBoundaryDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a770dc",
   "metadata": {},
   "source": [
    "We load the iris data from scikit learn, the data contains information about the Iris plant, there are three types/classes: Setosa, Versicolor, and Virginica. The data contains four features: sepal length, sepal width, petal length, and petal width all in centimeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a846a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "C = iris.target\n",
    "print(f'Iris types : {iris.target_names}')\n",
    "print(f'Features : {iris.feature_names}')\n",
    "print(\"Number of datapoints:\", len(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a7c5f",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Make a plot to visualize the data. To do so, we will choose two features from the four possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05246eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = 0\n",
    "feature2 = 1\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# ...\n",
    "\n",
    "plt.xlabel(iris.feature_names[feature1])\n",
    "plt.ylabel(iris.feature_names[feature2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6b9da",
   "metadata": {},
   "source": [
    "### Test/Train Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f256fcd",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Perform a Test/Train split by leaving 25 data samples for testing. For that, we will use the function `np.random.permutations()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d09b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtrain =\n",
    "# Ctrain =\n",
    "# Xtest = \n",
    "# Ctest = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0e086",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)\n",
    "The function below is used to plot the decision boundaries predicted by a model, together with the datapoints in the 2D plane (sepal length, sepal width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc598651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundaries(model, X, C):                                                                      \n",
    "    _, ax = plt.subplots()\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        model,\n",
    "        X[:, :2],\n",
    "        ax=ax,\n",
    "        cmap='cool',\n",
    "        response_method=\"predict\",\n",
    "        plot_method=\"pcolormesh\",\n",
    "        xlabel=iris.feature_names[0],\n",
    "        ylabel=iris.feature_names[1],\n",
    "        shading=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(\n",
    "        X[:, 0],\n",
    "        X[:, 1],\n",
    "        c=C,\n",
    "        cmap='cool',\n",
    "        edgecolors='k'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581873cd",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Use sklearn to fit an SVM model, use the first two feature vectors only and plot the decision boundaries. <br>\n",
    "Change the kernel to check how that affects the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae7c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for a linear kernel\n",
    "svc1 = svm.SVC(kernel='linear')\n",
    "svc1.fit(Xtrain[:, :2], Ctrain)\n",
    "plot_decision_boundaries(svc1, Xtrain, Ctrain)\n",
    "plt.title(\"3-Class classification SVM linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b426b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0a4bf3c",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Use the trained model to predict the labels of the testing set, and evaluate the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952eecef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bc02d69",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf87b669",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "We now use the KNN model. Fit the first two features of the data and plot the boundaries again for this model. \n",
    "This time change the neighbors parameter to see how that affects the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e188cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit a nearest-neighbor classifier\n",
    "n_neighbors = 3\n",
    "knn = KNeighborsClassifier(n_neighbors, weights='uniform')\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4b5e0f",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671e34d1",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Same question for a Gaussian naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9124c763",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763959f2",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ab316",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "Same question for a decision tree model (playing with the parameter `max_depth`). \n",
    "Use all the features this time, and create a graph that shows the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb84be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dectree = tree.DecisionTreeClassifier(max_depth=2)\n",
    "dectree.fit(Xtrain, Ctrain)\n",
    "tree.plot_tree(dectree);\n",
    "\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyPython",
   "language": "python",
   "name": "mypython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
