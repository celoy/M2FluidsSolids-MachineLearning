{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Save/Load a model and GPU support\n",
    "\n",
    "Notebook inspired from the Udemy course \"PyTorch for Deep Learning with Python Bootcamp\" and and from the [Pytorch official tutorials](https://pytorch.org/tutorials/).\n",
    "\n",
    "In this section we'll:\n",
    "\n",
    "* learn how to save and load a trained model\n",
    "* have a quick look at GPU support\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features=4, h1=8, h2=9, out_features=3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features,h1)    # input layer\n",
    "        self.fc2 = nn.Linear(h1, h2)            # hidden layer\n",
    "        self.out = nn.Linear(h2, out_features)  # output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model to a file\n",
    "Once that you have a trained model that seems to work correctly classify, you may want to save it to disk.\n",
    "\n",
    "To do so, the tools we'll use are <a href='https://pytorch.org/docs/stable/torch.html#torch.save'><strong><tt>torch.save()</tt></strong></a> and <a href='https://pytorch.org/docs/stable/torch.html#torch.load'><strong><tt>torch.load()</tt></strong></a><br>\n",
    "\n",
    "There are two basic ways to save a model.<br>\n",
    "\n",
    "The first saves/loads the `state_dict` (learned parameters) of the model, but not the model class. The syntax follows:<br>\n",
    "<tt><strong>Save:</strong>&nbsp;torch.save(model.state_dict(), PATH)<br><br>\n",
    "<strong>Load:</strong>&nbsp;model = TheModelClass(\\*args, \\*\\*kwargs)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model.load_state_dict(torch.load(PATH))<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model.eval()</tt>\n",
    "\n",
    "The second saves the entire model including its class and parameters as a pickle file. Care must be taken if you want to load this into another notebook to make sure all the target data is brought in properly.<br>\n",
    "<tt><strong>Save:</strong>&nbsp;torch.save(model, PATH)<br><br>\n",
    "<strong>Load:</strong>&nbsp;model = torch.load(PATH))<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model.eval()</tt>\n",
    "\n",
    "In either method, you must call <tt>model.eval()</tt> to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results.\n",
    "\n",
    "For more information visit https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train a model\n",
    "\n",
    "The first thing to do is to call an istance of the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we just create a model from our `Model()` class. Imagine that we already trained it and achieved a reasonable performance.\n",
    "\n",
    "Now it is time to save it.\n",
    "\n",
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'TrainedModel.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a new model\n",
    "We'll load a new model object and test it as we had before to make sure it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Model()\n",
    "new_model.load_state_dict(torch.load('TrainedModel.pt'))\n",
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLEwUd78LOEA"
   },
   "source": [
    "## GPU support\n",
    "\n",
    "A crucial feature of PyTorch is the support of GPUs, short for Graphics Processing Unit. A GPU can perform many thousands of small operations in parallel, making it very well suitable for performing large matrix operations in neural networks. \n",
    "\n",
    "CPUs and GPUs have both different advantages and disadvantages (see [this post](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/) about main differences between the two) which is why many computers contain both components and use them for different tasks. In case you are not familiar with GPUs, you can read up more details in this [NVIDIA blog post](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/) or [here](https://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html). \n",
    "\n",
    "GPUs can accelerate the training of your network up to a factor of $100$ which is essential for large neural networks. They can indeed speed up simple operations like matrix multiplication of a factor $50$.\n",
    "\n",
    "PyTorch implements a lot of functionality for supporting GPUs (mostly those of NVIDIA due to the libraries [CUDA](https://developer.nvidia.com/cuda-zone) and [cuDNN](https://developer.nvidia.com/cudnn)). \n",
    "\n",
    "First, let's check whether you have a GPU available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3txXvGN3LOEB",
    "outputId": "1da7999b-261d-4481-e00f-ae085e49b267"
   },
   "outputs": [],
   "source": [
    "gpu_avail = torch.cuda.is_available()\n",
    "print(f\"Is the GPU available? {gpu_avail}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jolpilgLOEC"
   },
   "source": [
    "If you have a GPU on your computer but the command above returns False, make sure you have the correct CUDA-version installed.  On Google Colab, make sure that you have selected a GPU in your runtime setup (in the menu, check under `Runtime -> Change runtime type`). \n",
    "\n",
    "By default, all tensors you create are stored on the CPU. We can push a tensor to the GPU by using the function `.to(...)`, or `.cuda()`. However, it is often a good practice to define a `device` object in your code which points to the GPU if you have one, and otherwise to the CPU. Then, you can write your code with respect to this device object, and it allows you to run the same code on both a CPU-only system, and one with a GPU. Let's try it below. We can specify the device as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ke1kxBFyLOEC",
    "outputId": "6a137558-2c51-42ef-eca1-5978309f3676"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RwvvTAHLOED"
   },
   "source": [
    "Now let's create a tensor and push it to the device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHjBTXHZLOED",
    "outputId": "38033f8f-dc7c-4245-fe5b-8f027d4c51ca"
   },
   "outputs": [],
   "source": [
    "x = torch.zeros(2, 3)\n",
    "x = x.to(device)\n",
    "print(\"X\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6OOwgoLLOED"
   },
   "source": [
    "In case you have a GPU, you should now see the attribute `device='cuda:0'` being printed next to your tensor. The zero next to cuda indicates that this is the zero-th GPU device on your computer. PyTorch also supports multi-GPU systems, but this you will only need once you have very big networks to train (if interested, see the [PyTorch documentation](https://pytorch.org/docs/stable/distributed.html#distributed-basics)).\n",
    "\n",
    "Operation between tensors can happen only if they are on the same device. Aslo, a model can be applied to a tensor only if they are both on the same device. So for example when using a model trained on GPU using Colab remember to set the accordingly the device on your local machine or to push the model on your CPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cPDabQhLOEE"
   },
   "source": [
    "## Set the GPU seed\n",
    "When generating random numbers, the seed between CPU and GPU is not synchronized. Hence, we need to set the seed on the GPU separately to ensure a reproducible code. Note that due to different GPU architectures, running the same code on different GPUs does not guarantee the same random numbers. Still, we don't want that our code gives us a different output every time we run it on the exact same hardware. \n",
    "\n",
    "Hence, we also set the seed on the GPU as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7ft-OVtLOEF"
   },
   "outputs": [],
   "source": [
    "# GPU operations have a separate seed we also want to set\n",
    "if torch.cuda.is_available(): \n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
